{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import yaml\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the configuration from the YAML file\n",
    "def load_config(config_file):\n",
    "    with open(config_file, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read csv files sequentially from the directory as dataframe\n",
    "def process_files(base_dir):\n",
    "    for root, dirs, files in os.walk(base_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('csv'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                df = pd.read_csv(file_path)\n",
    "                yield df, file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_unchanged_files(config, main_folder, output_path):\n",
    "    # Process unchanged files\n",
    "    unchanged_files = config['parameters']['unchanged_files']\n",
    "    print(f\"Unchanged files: {unchanged_files}\")\n",
    "    for df, file_path in process_files(main_folder):\n",
    "        param_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "        if param_name in unchanged_files:\n",
    "            #print(f\"Skipping {param_name} as it is in the no_change_parameters list.\")\n",
    "            file_path = os.path.join(output_path, f'{param_name}.csv')\n",
    "            df.to_csv(file_path)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_changed_files(config, main_folder, output_path):\n",
    "    changed_files = config['parameters']['changed_files'] # list of file with parameters\n",
    "\n",
    "    for df, file_path in process_files(main_folder):\n",
    "        param_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "       \n",
    "        file_path = os.path.join(output_path, f'{param_name}.csv')\n",
    "        for file in changed_files:\n",
    "            if param_name == file['file_name']: \n",
    "                method = file['method']\n",
    "                index_count = file['index_count']\n",
    "                regions_to_split = file['regions_to_split']\n",
    "                new_regions = file['new_regions']\n",
    "                split_ratio = file['split_ratio']\n",
    "                print(param_name, method, index_count, regions_to_split, new_regions, split_ratio)\n",
    "                \n",
    "                df.set_index(df.columns[:index_count].tolist(), inplace=True)\n",
    "                \n",
    "                for region in regions_to_split: \n",
    "                    if region in df.index.get_level_values('Region'):\n",
    "                        print(\"True\")\n",
    "                        region_data = df.loc[region]\n",
    "                        \n",
    "                        new_region_dfs = []\n",
    "                        for new_region, ratio in split_ratio.items():\n",
    "                            if index_count == 1:\n",
    "                                new_data = region_data.to_frame().T if isinstance(region_data, pd.Series) else region_data.copy()\n",
    "                                new_data['Value'] *= ratio\n",
    "                                new_data['Region'] = new_region\n",
    "                                new_data.reset_index(inplace=True, drop=True)\n",
    "                                new_region_dfs.append(new_data)\n",
    "                            else:\n",
    "                                new_data = region_data.copy()\n",
    "                                new_data['Value'] *= ratio\n",
    "                                new_data['Region'] = new_region\n",
    "                                new_data.reset_index(inplace=True) \n",
    "                                new_region_dfs.append(new_data) \n",
    "                        combined_new_regions = pd.concat(new_region_dfs, ignore_index=True)\n",
    "                        df.reset_index(inplace=True)\n",
    "                        df = pd.concat([df, combined_new_regions], ignore_index=True)\n",
    "\n",
    "                        df.set_index(df.columns[:index_count].tolist(), inplace=True)\n",
    "                        df.to_csv(file_path)   \n",
    "                    else:\n",
    "                        print(\"do not have region in dataset\")\n",
    "                        df.to_csv(file_path)    \n",
    "\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trade_files(config, main_folder, output_path, trade_connection):\n",
    "    trade_changed_files = config['parameters']['trade_files']  \n",
    "    \n",
    "    for df, file_path in process_files(main_folder):\n",
    "        param_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "        file_path = os.path.join(output_path, f'{param_name}.csv') \n",
    "        \n",
    "        for file in trade_changed_files:\n",
    "            if param_name == file['file_name']:\n",
    "                method = file['method']\n",
    "                index_count = file['index_count']\n",
    "                regions_to_split = file['regions_to_split']\n",
    "                new_regions = file['new_regions']\n",
    "                split_ratio = file['split_ratio']\n",
    "                new_connections = file.get('new_connections', {})\n",
    "                \n",
    "                print(param_name, method, index_count, regions_to_split, new_regions, split_ratio, new_connections)\n",
    "                \n",
    "                df.set_index(df.columns[:index_count].tolist(), inplace=True)\n",
    "\n",
    "                for region in regions_to_split:\n",
    "                    if region in df.index.get_level_values('Region'):\n",
    "                        print(f\"Processing region: {region}\")\n",
    "                        ref_row = df.loc[region].iloc[0].to_dict() \n",
    "                        \n",
    "                        region_data = df.loc[region].copy() #added later\n",
    "                        index_cols = df.index.names #added later\n",
    "                        new_region_dfs = []\n",
    "                        \n",
    "                        for new_region, ratio in split_ratio.items():\n",
    "                            new_data = region_data.to_frame().T if isinstance(region_data, pd.Series) else region_data.copy()\n",
    "                            new_data['Value'] *= ratio\n",
    "                            new_data.loc[:, \"Region\"] = new_region\n",
    "                            new_data.reset_index(inplace=True)\n",
    "                            new_region_dfs.append(new_data)\n",
    "                        \n",
    "                        \"\"\"# Add new connections dynamically\n",
    "                        for key, details in new_connections.items():\n",
    "                            if \"-\" in key:\n",
    "                                reg1, reg2 = key.split(\"-\")\n",
    "\n",
    "                                # Manually create a new row for this connection\n",
    "                                conn_row = ref_row.copy()\n",
    "\n",
    "                                conn_row[index_cols[0]] = reg1  # Update first index column (e.g., 'Region')\n",
    "                                if len(index_cols) > 1:\n",
    "                                    conn_row[index_cols[1]] = reg2  # Update second index column (e.g., 'Region2')\n",
    "\n",
    "                                conn_row[\"Value\"] = details[\"value\"]  # Assign new connection value\n",
    "\n",
    "                                # Convert to DataFrame and add to list\n",
    "                                new_region_dfs.append(pd.DataFrame([conn_row]))\n",
    "                                print(f\"Added connection: {reg1} -> {reg2} with Value: {details['value']}\")\"\"\"\n",
    "                        \n",
    "                        # Add new connections from excel sheet\n",
    "                        # Load workbook\n",
    "                        xls = pd.ExcelFile(trade_connection)\n",
    "                        df_connections = pd.DataFrame()\n",
    "                        for sheet_name in xls.sheet_names:\n",
    "                            if sheet_name == param_name:\n",
    "                                df_connections = pd.read_excel(xls, sheet_name=sheet_name)\n",
    "\n",
    "                        combined_new_regions = pd.concat(new_region_dfs, ignore_index=True)\n",
    "                        combined_new_regions = pd.concat([combined_new_regions,df_connections], ignore_index=True)\n",
    "                        df.reset_index(inplace=True)\n",
    "                        df = pd.concat([df, combined_new_regions], ignore_index=True)\n",
    "                        df.set_index(df.columns[:index_count].tolist(), inplace=True)\n",
    "                        df.to_csv(file_path)\n",
    "                    else:\n",
    "                        print(f\"Region {region} not found in dataset\")\n",
    "                        df.to_csv(file_path)\n",
    "\n",
    "    print(\"Processing completed.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "def special_files(config, main_folder, output_path):\n",
    "    changed_files = config['parameters']['special_files']\n",
    "    \n",
    "    # Assuming a function process_files that yields dataframes and their file paths\n",
    "    for df, file_path in process_files(main_folder):\n",
    "        param_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "        output_file_path = os.path.join(output_path, f'{param_name}.csv')\n",
    "        \n",
    "        for file in changed_files:\n",
    "            if param_name == file['file_name']:\n",
    "                index_count = file['index_count']\n",
    "                regions_to_split = file['regions_to_split']\n",
    "                new_regions = file['new_regions']\n",
    "                split_factors = file['split_factors']\n",
    "                \n",
    "                df.set_index(df.columns[:index_count].tolist(), inplace=True)\n",
    "                \n",
    "                # Process each specified region for splitting\n",
    "                for region in regions_to_split:\n",
    "                    if region in df.index.get_level_values('Region'):\n",
    "                        region_data = df.loc[region]\n",
    "                        \n",
    "                        # Handle different types of data (Series or DataFrame)\n",
    "                        if isinstance(region_data, pd.Series):\n",
    "                            region_data = region_data.to_frame().T\n",
    "                        \n",
    "                        new_region_dfs = []\n",
    "                        \n",
    "                        # Iterate over each fuel type and its respective split factors for new regions\n",
    "                        for fuel_type, regions_factors in split_factors.items():\n",
    "                            # Filter data by fuel type\n",
    "                            fuel_data = region_data[region_data['Fuel'] == fuel_type]\n",
    "                            \n",
    "                            for new_region, factor in regions_factors.items():\n",
    "                                new_data = fuel_data.copy()\n",
    "                                new_data['Value'] *= factor\n",
    "                                new_data['Region'] = new_region\n",
    "                                new_data.reset_index(drop=True, inplace=True) \n",
    "                                new_region_dfs.append(new_data)\n",
    "                        \n",
    "\n",
    "                    # Combine all new data and add it back to the main DataFrame\n",
    "                    combined_new_regions = pd.concat(new_region_dfs, ignore_index=True)\n",
    "                    combined_new_regions.to_csv('combined_new_regions.csv')\n",
    "                    df.reset_index(inplace=True)\n",
    "                    df = pd.concat([df, combined_new_regions], ignore_index=True)\n",
    "                    print(f'Final dataframe:', df.head(2))\n",
    "\n",
    "                    # Remove old region data\n",
    "                    #df = df[df['Region'] != region]\n",
    "                    \n",
    "                    # Reapply multi-index  \n",
    "                    df.set_index(df.columns[:index_count].tolist(), inplace=True)\n",
    "\n",
    "                    # Save the DataFrame to the constructed path\n",
    "                    df.to_csv(output_file_path, index=True)   \n",
    "\n",
    "                else:\n",
    "                    print(\"do not have region in dataset\")\n",
    "                    df.to_csv(output_file_path)    \n",
    "                \n",
    "    print(\"Processing completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeseries_files(config, main_folder, output_path):\n",
    "    ts_files = config['parameters']['time_series_files'] \n",
    "    for df, file_path in process_files(main_folder):\n",
    "        df.columns = df.iloc[0]\n",
    "        df = df[1:]\n",
    "        param_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "        file_path = os.path.join(output_path, f'{param_name}.csv')\n",
    "        for file in ts_files:\n",
    "            if param_name == file['file_name']: \n",
    "                regions_to_split = file['regions_to_split']\n",
    "                new_regions = file['new_regions']\n",
    "                split_ratio = file['split_ratio']\n",
    "                print(param_name, regions_to_split, new_regions, split_ratio)\n",
    "                \n",
    "                for region in new_regions:\n",
    "                    df[region] = df[regions_to_split].sum(axis=1) * split_ratio[region]\n",
    "            df.to_csv(file_path)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "base_dir = \"/Users/shwetat/Projects/Genesys-mod_data_repo/GENeSYS_MOD.data/Data/Parameters\"\n",
    "base_dir_t = \"/Users/shwetat/Projects/Genesys-mod_data_repo/GENeSYS_MOD.data/Data/Timeseries\"\n",
    "output_dir = \"/Users/shwetat/Projects/Genesys-mod_data_repo/GENeSYS_MOD.data/DataNew\"\n",
    "config = load_config(\"config_disaggregation.yaml\")\n",
    "trade_connection = '/Users/shwetat/Projects/Genesys-mod_data_repo/GENeSYS_MOD.data/Aggregate Disaggregate Scripts/routeSettings.xlsx'\n",
    "process_unchanged_files(config, base_dir, output_dir)\n",
    "process_changed_files(config, base_dir, output_dir)\n",
    "trade_files(config, base_dir, output_dir, trade_connection)\n",
    "timeseries_files(config, base_dir_t, output_dir)\n",
    "#special_files(config, base_dir, output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "havnett",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
