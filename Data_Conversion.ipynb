{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee139be2-2d6f-46d1-adcf-fcd5eb9a6cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5792e138-d7b1-4911-bc97-497db1673947",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_csv_directory = 'output_csv'\n",
    "output_excel_directory = 'output_excel'\n",
    "sets_and_tags_directory = '00_Sets&Tags'\n",
    "parameter_directory = '00_Parameters'\n",
    "excel_file_path = 'GENeSYS-MOD_User_Input_Settings_v07_kh_06-10-2023.xlsx'\n",
    "current_directory = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9942077d-88f0-41ea-82b3-a22a06ad58c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_settings_file(file_path):\n",
    "    # Open the Excel file\n",
    "    xls = pd.ExcelFile(excel_file_path, engine='openpyxl')\n",
    "    \n",
    "    # Get the list of sheet names in the Excel file\n",
    "    sheets_to_read = xls.sheet_names\n",
    "    \n",
    "    # Initialize an empty dictionary to store DataFrames\n",
    "    data_frames = {}\n",
    "    filtered_df = {}\n",
    "    unique_values = {}\n",
    "    \n",
    "    unique_values_concatenated = pd.DataFrame()\n",
    "    column_list = []\n",
    "    \n",
    "    # Read sheets and store them in the dictionary\n",
    "    for sheet_name in sheets_to_read:\n",
    "        data_frames = xls.parse(sheet_name)\n",
    "    \n",
    "        filtered_df= data_frames[data_frames.iloc[:, 1] == 1] # Assuming the second column is indexed at 1 (0-based index)\n",
    "    \n",
    "        column_list.append(filtered_df.columns[0]) # collect column header for each set sheet\n",
    "       \n",
    "        unique_values= pd.DataFrame(filtered_df.iloc[:, 0].unique())  # Assuming the first column is indexed at 0 (0-based index)\n",
    "        unique_values_parameter = pd.DataFrame(unique_values)\n",
    "        \n",
    "        unique_values_concatenated = pd.concat([unique_values_concatenated, unique_values], axis=1)\n",
    "    \n",
    "    # Close the Excel file\n",
    "    xls.close()    \n",
    "        \n",
    "    # Need to put header to the dataframe\n",
    "    unique_values_concatenated.columns = column_list\n",
    "    \n",
    "    # Create a CSV file containing unique values\n",
    "    unique_values_csv_file_path = os.path.join(output_csv_directory, 'Sets.csv')\n",
    "    unique_values_concatenated.to_csv(unique_values_csv_file_path, index=False, decimal='.') \n",
    "    \n",
    "    if \"Region\" in unique_values_concatenated.columns:\n",
    "        unique_values_concatenated[\"Region2\"] = unique_values_concatenated[\"Region\"]\n",
    "    # Return the concatenated DataFrame of unique values\n",
    "    return unique_values_concatenated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "664dbbff-0a80-4319-9f30-d2333d4494f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_regular_parameters(main_directory, file_pattern='Par_'):\n",
    "    filepaths = []\n",
    "\n",
    "    # Updated path for the sets_and_tags_directory within the 00_Parameters directory\n",
    "    sets_and_tags_directory_f = os.path.join(main_directory, parameter_directory, sets_and_tags_directory)\n",
    "    \n",
    "    # Check if the sets_and_tags_directory exists, then add files\n",
    "    if os.path.exists(sets_and_tags_directory_f):\n",
    "        for f in os.listdir(sets_and_tags_directory_f):\n",
    "            if f.startswith(file_pattern) and f.endswith('.csv'):\n",
    "                filepaths.append(os.path.join(sets_and_tags_directory_f, f))\n",
    "\n",
    "    # Continue to add files from subdirectories of 00_Parameters\n",
    "    parameters_directory_f = os.path.join(main_directory, parameter_directory)\n",
    "    for root, dirs, files in os.walk(parameters_directory_f):\n",
    "        for file in files:\n",
    "            if file.startswith(file_pattern) and file.endswith('.csv'):\n",
    "                filepaths.append(os.path.join(root, file))\n",
    "\n",
    "    return filepaths\n",
    "\n",
    "\n",
    "\n",
    "def process_regular_parameter(csv_file_path, unique_values_concatenated):\n",
    "    # Compute and truncate worksheet_name to ensure it doesn't exceed 31 characters\n",
    "    worksheet_name = os.path.splitext(os.path.basename(csv_file_path))[0]\n",
    "    if len(worksheet_name) > 31:\n",
    "        worksheet_name = worksheet_name[:31]\n",
    "\n",
    "    # Read the CSV file into a Pandas DataFrame\n",
    "    df = pd.read_csv(csv_file_path, delimiter=',')\n",
    "\n",
    "    # Data conversions and handling NaNs\n",
    "    if 'Year' in df.columns:\n",
    "        df['Year'] = pd.to_numeric(df['Year'], errors='coerce', downcast='integer')\n",
    "        df = df.dropna(subset=['Year'])  # Dropping NaNs in 'Year'\n",
    "\n",
    "    if 'Mode_of_operation' in df.columns:\n",
    "        df['Mode_of_operation'] = df['Mode_of_operation'].astype('Int64', errors='ignore')\n",
    "        df = df.dropna(subset=['Mode_of_operation'])  # Dropping NaNs in 'Mode_of_operation'\n",
    "\n",
    "    # Rename columns with .1, .2, etc. naming convention\n",
    "    for col in df.columns:\n",
    "        if '.' in col:\n",
    "            base_name, counter = col.split('.')\n",
    "            new_col_name = f\"{base_name}{int(counter) + 1}\"  # Add 1 because we start from the first duplicate\n",
    "            df.rename(columns={col: new_col_name}, inplace=True)\n",
    "\n",
    "    # Filter DataFrame based on unique_values_concatenated\n",
    "    columns_to_keep = [col for col in df.columns if col in unique_values_concatenated.columns or col == 'Value']\n",
    "    df = df[columns_to_keep]\n",
    "\n",
    "    for header in unique_values_concatenated.columns:\n",
    "        if header in df.columns:\n",
    "            df = df[df[header].isin(unique_values_concatenated[header])]\n",
    "\n",
    "    # Determine the pivot column\n",
    "    pivot_column = 'Region2' if 'Region2' in df.columns and 'Region' in df.columns else 'Year'\n",
    "\n",
    "    # Pivot the DataFrame if the pivot column exists\n",
    "    if pivot_column in df.columns:\n",
    "        # Pivot the DataFrame\n",
    "        df_pivot = df.pivot(index=[col for col in df.columns if col not in [pivot_column, 'Value']],\n",
    "                            columns=pivot_column, values='Value').reset_index()\n",
    "\n",
    "        # Flatten MultiIndex columns (if any)\n",
    "        df_pivot.columns = ['_'.join(map(str, col)).strip() if isinstance(col, tuple) else str(col) for col in df_pivot.columns.values]\n",
    "\n",
    "        # Replace NaNs with empty strings for better readability\n",
    "        df_pivot.replace('nan', '', inplace=True)\n",
    "    else:\n",
    "        df_pivot = df  # If pivot_column is not in df, use original DataFrame\n",
    "\n",
    "    return df, df_pivot, worksheet_name\n",
    "\n",
    "\n",
    "#    return df, worksheet_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7148359e-8074-4a99-9525-e10897184955",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_regular_parameters(df, worksheet_name, output_directory, writer=None, output_format='excel'):\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    if output_format == 'csv':\n",
    "        # Define the path for the CSV file\n",
    "        output_file_path = os.path.join(output_directory, f\"{worksheet_name}.csv\")\n",
    "        # Write DataFrame to CSV\n",
    "        df.to_csv(output_file_path, index=False)\n",
    "\n",
    "    elif output_format == 'excel' and writer is not None:\n",
    "        # Check if the sheet exists and remove it\n",
    "        if worksheet_name in writer.book.sheetnames:\n",
    "            del writer.book[worksheet_name]\n",
    "        # Write DataFrame to Excel using the provided writer\n",
    "        df.to_excel(writer, sheet_name=worksheet_name, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a47b9af-9dbf-44f8-bb6f-5d3203f81fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure unique_values_concatenated is defined\n",
    "unique_values_concatenated = read_settings_file(excel_file_path)\n",
    "\n",
    "# Define the output format - can be 'csv' or 'excel'\n",
    "output_format = 'csv'  # Change this to 'csv' if you want CSV output\n",
    "\n",
    "# Process each regular parameter file\n",
    "regular_parameter_paths = read_regular_parameters(current_directory)\n",
    "\n",
    "# Store the worksheet names and corresponding dataframes\n",
    "worksheets_data = {}\n",
    "\n",
    "if output_format == 'excel':\n",
    "    output_excel_file_path = os.path.join(output_excel_directory, 'output.xlsx')\n",
    "\n",
    "    # Process files and store dataframes with their names\n",
    "    for path in regular_parameter_paths:\n",
    "        df_original, df_pivot, worksheet_name = process_regular_parameter(path, unique_values_concatenated)\n",
    "        worksheets_data[worksheet_name] = df_pivot\n",
    "\n",
    "    # Sort worksheet names alphabetically, excluding 'Sets'\n",
    "    sorted_worksheets = sorted([name for name in worksheets_data if name != 'Sets'])\n",
    "\n",
    "    # Write to Excel file\n",
    "    with pd.ExcelWriter(output_excel_file_path, engine='openpyxl') as excel_writer:\n",
    "        # Write 'Sets' sheet first\n",
    "        unique_values_concatenated.to_excel(excel_writer, sheet_name='Sets', index=False)\n",
    "\n",
    "        # Write other sheets in alphabetical order\n",
    "        for worksheet_name in sorted_worksheets:\n",
    "            df_pivot = worksheets_data[worksheet_name]\n",
    "            df_pivot.to_excel(excel_writer, sheet_name=worksheet_name, index=False)\n",
    "\n",
    "else:\n",
    "    # CSV output processing (same as before)\n",
    "    for path in regular_parameter_paths:\n",
    "        df_original, df_pivot, worksheet_name = process_regular_parameter(path, unique_values_concatenated)\n",
    "        try:\n",
    "            output_csv_file_path = os.path.join(output_csv_directory, f\"{worksheet_name}.csv\")\n",
    "            df_original.to_csv(output_csv_file_path, index=False)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0557f7af-60a3-408e-b405-7e2225a64be7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ac3265-7afa-468f-a4e6-8ba033a882df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
