{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee139be2-2d6f-46d1-adcf-fcd5eb9a6cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e422fd9-87f3-4553-8408-f277c20b66e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current working directory (where the script is located)\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Assuming the script is in 'Conversion Script', and you need to go one level up to access 'Data' and 'Output'\n",
    "parent_directory = os.path.dirname(current_directory)\n",
    "\n",
    "# Directories inside the 'Data' folder\n",
    "data_directory = os.path.join(parent_directory, 'Data')\n",
    "sets_and_tags_directory = os.path.join(data_directory, 'Sets&Tags')\n",
    "parameter_directory = os.path.join(data_directory, 'Parameters')\n",
    "timeseries_directory = os.path.join(data_directory, 'Timeseries')\n",
    "\n",
    "# Directories for output\n",
    "output_directory = os.path.join(parent_directory, 'Output')\n",
    "output_csv_directory = os.path.join(output_directory, 'output_csv')\n",
    "output_excel_directory = os.path.join(output_directory, 'output_excel')\n",
    "output_excel_file_path = os.path.join(output_excel_directory, 'output.xlsx')\n",
    "output_excel_file_path_timeseries = os.path.join(output_excel_directory, 'output_timeseries.xlsx')\n",
    "\n",
    "# Path to the Excel settings file\n",
    "excel_file_path = os.path.join(current_directory, 'Set_filter_file.xlsx')\n",
    "\n",
    "# Output format\n",
    "output_file_format = 'csv'  # Change this to 'csv' or 'excel' depending on your needs\n",
    "output_format = 'long' # Change to 'wide' or 'long' depending on your needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9942077d-88f0-41ea-82b3-a22a06ad58c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_settings_file(file_path):\n",
    "    # Open the Excel file using the provided file path\n",
    "    xls = pd.ExcelFile(file_path, engine='openpyxl')\n",
    "    \n",
    "    # Get the list of sheet names in the Excel file\n",
    "    sheets_to_read = xls.sheet_names\n",
    "    \n",
    "    unique_values_concatenated = pd.DataFrame()\n",
    "    column_list = []\n",
    "    \n",
    "    # Read sheets and store them in the dictionary\n",
    "    for sheet_name in sheets_to_read:\n",
    "        data_frame = xls.parse(sheet_name)\n",
    "    \n",
    "        filtered_df = data_frame[data_frame.iloc[:, 1] == 1] # Assuming the second column is indexed at 1 (0-based index)\n",
    "    \n",
    "        column_list.append(filtered_df.columns[0]) # Collect column header for each set sheet\n",
    "       \n",
    "        unique_values = pd.DataFrame(filtered_df.iloc[:, 0].unique())  # Assuming the first column is indexed at 0 (0-based index)\n",
    "        \n",
    "        unique_values_concatenated = pd.concat([unique_values_concatenated, unique_values], axis=1)\n",
    "    \n",
    "    # Close the Excel file\n",
    "    xls.close()    \n",
    "    \n",
    "    # Need to put header to the DataFrame\n",
    "    unique_values_concatenated.columns = column_list\n",
    "    \n",
    "    # Create a CSV file containing unique values\n",
    "    unique_values_csv_file_path = os.path.join(output_csv_directory, 'Sets.csv')\n",
    "    unique_values_concatenated.to_csv(unique_values_csv_file_path, index=False, decimal='.') \n",
    "    \n",
    "    if \"Region\" in unique_values_concatenated.columns:\n",
    "        unique_values_concatenated[\"Region2\"] = unique_values_concatenated[\"Region\"]\n",
    "    \n",
    "    # Return the concatenated DataFrame of unique values\n",
    "    return unique_values_concatenated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "664dbbff-0a80-4319-9f30-d2333d4494f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_regular_parameters(main_directory, file_pattern='Par_'):\n",
    "    filepaths = []\n",
    "\n",
    "    # Updated path for the sets_and_tags_directory within the 00_Parameters directory\n",
    "    sets_and_tags_directory_f = os.path.join(main_directory, parameter_directory, sets_and_tags_directory)\n",
    "    \n",
    "    # Check if the sets_and_tags_directory exists, then add files\n",
    "    if os.path.exists(sets_and_tags_directory_f):\n",
    "        for f in os.listdir(sets_and_tags_directory_f):\n",
    "            if f.startswith(file_pattern) and f.endswith('.csv'):\n",
    "                filepaths.append(os.path.join(sets_and_tags_directory_f, f))\n",
    "\n",
    "    # Continue to add files from subdirectories of 00_Parameters\n",
    "    parameters_directory_f = os.path.join(main_directory, parameter_directory)\n",
    "    for root, dirs, files in os.walk(parameters_directory_f):\n",
    "        for file in files:\n",
    "            if file.startswith(file_pattern) and file.endswith('.csv'):\n",
    "                filepaths.append(os.path.join(root, file))\n",
    "\n",
    "    return filepaths\n",
    "\n",
    "\n",
    "\n",
    "def process_regular_parameter(csv_file_path, unique_values_concatenated):\n",
    "    # Compute and truncate worksheet_name to ensure it doesn't exceed 31 characters\n",
    "    worksheet_name = os.path.splitext(os.path.basename(csv_file_path))[0]\n",
    "    if len(worksheet_name) > 31:\n",
    "        worksheet_name = worksheet_name[:31]\n",
    "\n",
    "    # Read the CSV file into a Pandas DataFrame\n",
    "    df = pd.read_csv(csv_file_path, delimiter=',')\n",
    "\n",
    "    # Data conversions and handling NaNs\n",
    "    if 'Year' in df.columns:\n",
    "        df['Year'] = pd.to_numeric(df['Year'], errors='coerce', downcast='integer')\n",
    "        df = df.dropna(subset=['Year'])  # Dropping NaNs in 'Year'\n",
    "\n",
    "    if 'Mode_of_operation' in df.columns:\n",
    "        df['Mode_of_operation'] = df['Mode_of_operation'].astype('Int64', errors='ignore')\n",
    "        df = df.dropna(subset=['Mode_of_operation'])  # Dropping NaNs in 'Mode_of_operation'\n",
    "\n",
    "    # Rename columns with .1, .2, etc. naming convention\n",
    "    for col in df.columns:\n",
    "        if '.' in col:\n",
    "            base_name, counter = col.split('.')\n",
    "            new_col_name = f\"{base_name}{int(counter) + 1}\"  # Add 1 because we start from the first duplicate\n",
    "            df.rename(columns={col: new_col_name}, inplace=True)\n",
    "\n",
    "    # Filter DataFrame based on unique_values_concatenated\n",
    "    columns_to_keep = [col for col in df.columns if col in unique_values_concatenated.columns or col == 'Value']\n",
    "    df = df[columns_to_keep]\n",
    "\n",
    "    for header in unique_values_concatenated.columns:\n",
    "        if header in df.columns:\n",
    "            df = df[df[header].isin(unique_values_concatenated[header])]\n",
    "\n",
    "    # Determine the pivot column\n",
    "    pivot_column = 'Region2' if 'Region2' in df.columns and 'Region' in df.columns else 'Year'\n",
    "\n",
    "    # Pivot the DataFrame if the pivot column exists\n",
    "    if pivot_column in df.columns:\n",
    "        # Pivot the DataFrame\n",
    "        df_pivot = df.pivot(index=[col for col in df.columns if col not in [pivot_column, 'Value']],\n",
    "                            columns=pivot_column, values='Value').reset_index()\n",
    "\n",
    "        # Flatten MultiIndex columns (if any)\n",
    "        df_pivot.columns = ['_'.join(map(str, col)).strip() if isinstance(col, tuple) else str(col) for col in df_pivot.columns.values]\n",
    "\n",
    "        # Replace NaNs with empty strings for better readability\n",
    "        df_pivot.replace('nan', '', inplace=True)\n",
    "    else:\n",
    "        df_pivot = df  # If pivot_column is not in df, use original DataFrame\n",
    "\n",
    "    return df, df_pivot, worksheet_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3e1380c-a70e-4f17-a5c3-b9719ed16ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_regular_parameters(main_directory, file_pattern='Par_'):\n",
    "    filepaths = []\n",
    "\n",
    "    # Updated path for the sets_and_tags_directory within the 00_Parameters directory\n",
    "    sets_and_tags_directory_f = os.path.join(main_directory, parameter_directory, sets_and_tags_directory)\n",
    "    \n",
    "    # Check if the sets_and_tags_directory exists, then add files\n",
    "    if os.path.exists(sets_and_tags_directory_f):\n",
    "        for f in os.listdir(sets_and_tags_directory_f):\n",
    "            if f.startswith(file_pattern) and f.endswith('.csv'):\n",
    "                filepaths.append(os.path.join(sets_and_tags_directory_f, f))\n",
    "\n",
    "    # Continue to add files from subdirectories of 00_Parameters\n",
    "    parameters_directory_f = os.path.join(main_directory, parameter_directory)\n",
    "    for root, dirs, files in os.walk(parameters_directory_f):\n",
    "        for file in files:\n",
    "            if file.startswith(file_pattern) and file.endswith('.csv'):\n",
    "                filepaths.append(os.path.join(root, file))\n",
    "\n",
    "    return filepaths\n",
    "\n",
    "\n",
    "\n",
    "def process_regular_parameter(csv_file_path, unique_values_concatenated):\n",
    "    # Compute and truncate worksheet_name to ensure it doesn't exceed 31 characters\n",
    "    worksheet_name = os.path.splitext(os.path.basename(csv_file_path))[0]\n",
    "    if len(worksheet_name) > 31:\n",
    "        worksheet_name = worksheet_name[:31]\n",
    "\n",
    "    # Read the CSV file into a Pandas DataFrame\n",
    "    df = pd.read_csv(csv_file_path, delimiter=',')\n",
    "\n",
    "    # Data conversions and handling NaNs\n",
    "    if 'Year' in df.columns:\n",
    "        df['Year'] = pd.to_numeric(df['Year'], errors='coerce', downcast='integer')\n",
    "        df = df.dropna(subset=['Year'])  # Dropping NaNs in 'Year'\n",
    "\n",
    "    if 'Mode_of_operation' in df.columns:\n",
    "        df['Mode_of_operation'] = df['Mode_of_operation'].astype('Int64', errors='ignore')\n",
    "        df = df.dropna(subset=['Mode_of_operation'])  # Dropping NaNs in 'Mode_of_operation'\n",
    "\n",
    "    # Rename columns with .1, .2, etc. naming convention\n",
    "    for col in df.columns:\n",
    "        if '.' in col:\n",
    "            base_name, counter = col.split('.')\n",
    "            new_col_name = f\"{base_name}{int(counter) + 1}\"  # Add 1 because we start from the first duplicate\n",
    "            df.rename(columns={col: new_col_name}, inplace=True)\n",
    "\n",
    "    # Filter DataFrame based on unique_values_concatenated\n",
    "    columns_to_keep = [col for col in df.columns if col in unique_values_concatenated.columns or col == 'Value']\n",
    "    df = df[columns_to_keep]\n",
    "\n",
    "    for header in unique_values_concatenated.columns:\n",
    "        if header in df.columns:\n",
    "            df = df[df[header].isin(unique_values_concatenated[header])]    \n",
    "    \n",
    "    # Initialize df_pivot\n",
    "    df_pivot = df  # Default to original DataFrame\n",
    "\n",
    "    if output_format == 'wide':\n",
    "        # Determine the pivot column\n",
    "        pivot_column = 'Region2' if 'Region2' in df.columns and 'Region' in df.columns else 'Year'    \n",
    "    \n",
    "        # Pivot the DataFrame if the pivot column exists\n",
    "        if pivot_column in df.columns:\n",
    "            # Pivot the DataFrame\n",
    "            df_pivot = df.pivot(index=[col for col in df.columns if col not in [pivot_column, 'Value']],\n",
    "                            columns=pivot_column, values='Value').reset_index()\n",
    "\n",
    "            # Flatten MultiIndex columns (if any)\n",
    "            df_pivot.columns = ['_'.join(map(str, col)).strip() if isinstance(col, tuple) else str(col) for col in df_pivot.columns.values]\n",
    "\n",
    "            # Replace NaNs with empty strings for better readability\n",
    "            df_pivot.replace('nan', '', inplace=True)\n",
    "\n",
    "    return df_pivot, worksheet_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20b012a9-cddc-48eb-9e39-1255c9fffe6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_regular_parameters(dataframes_dict, output_directory, output_file_format='excel'):\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    if output_file_format == 'excel':\n",
    "        \n",
    "        # Sort worksheet names alphabetically, but keep 'Sets' at the beginning\n",
    "        sorted_worksheet_names = ['Sets'] + sorted([name for name in dataframes_dict if name != 'Sets'])\n",
    "\n",
    "        # Write to Excel file\n",
    "        with pd.ExcelWriter(output_excel_file_path, engine='openpyxl', mode='w') as excel_writer:\n",
    "            for worksheet_name in sorted_worksheet_names:\n",
    "                df_to_output = dataframes_dict[worksheet_name]\n",
    "                df_to_output.to_excel(excel_writer, sheet_name=worksheet_name, index=False)\n",
    "    else:\n",
    "        # Handle CSV output if required (same as before)\n",
    "        for worksheet_name, df in dataframes_dict.items():\n",
    "            output_file_path = os.path.join(output_csv_directory, f\"{worksheet_name}.csv\")\n",
    "            df.to_csv(output_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fe578ba-9ca4-4988-9735-c81975a72a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure unique_values_concatenated is defined\n",
    "unique_values_concatenated = read_settings_file(excel_file_path)\n",
    "\n",
    "# Process each regular parameter file\n",
    "regular_parameter_paths = read_regular_parameters(current_directory)\n",
    "\n",
    "# Store the worksheet names and corresponding dataframes\n",
    "worksheets_data = {'Sets': unique_values_concatenated}  # Including 'Sets' sheet\n",
    "\n",
    "# Process files and store dataframes with their names\n",
    "for path in regular_parameter_paths:\n",
    "    df_pivot, worksheet_name = process_regular_parameter(path, unique_values_concatenated)\n",
    "    worksheets_data[worksheet_name] = df_pivot  # or df_original based on your requirement\n",
    "\n",
    "# Call the function to output data\n",
    "output_regular_parameters(worksheets_data, output_excel_directory, output_file_format)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0557f7af-60a3-408e-b405-7e2225a64be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_filter_timeseries(timeseries_dir, unique_values_concatenated):\n",
    "    filtered_data = {}\n",
    "\n",
    "    # Get the list of unique regions from unique_values_concatenated\n",
    "    unique_regions = unique_values_concatenated['Region'].unique()\n",
    "\n",
    "    # Iterate through each subdirectory in '00_Timeseries'\n",
    "    for subdir in os.listdir(timeseries_dir):\n",
    "        subdir_path = os.path.join(timeseries_dir, subdir)\n",
    "        if os.path.isdir(subdir_path):\n",
    "            # Assuming there is only one CSV file per subdirectory\n",
    "            csv_file = next((f for f in os.listdir(subdir_path) if f.endswith('.csv')), None)\n",
    "            if csv_file:\n",
    "                csv_path = os.path.join(subdir_path, csv_file)\n",
    "\n",
    "                # Read only the first row (excluding the first row of the file) to get the headers (regions)\n",
    "                headers = pd.read_csv(csv_path, skiprows=1, nrows=0)\n",
    "\n",
    "                # Include the first column (whatever it is) and filter the rest based on unique_regions\n",
    "                columns_to_keep = [headers.columns[0]] + [col for col in headers.columns[1:] if col in unique_regions]\n",
    "\n",
    "                # Now read the entire CSV with filtered columns, skipping the first row\n",
    "                df = pd.read_csv(csv_path, skiprows=1, usecols=columns_to_keep)\n",
    "\n",
    "                filtered_data[subdir] = df\n",
    "\n",
    "    return filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8ac3265-7afa-468f-a4e6-8ba033a882df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_timeseries_data(filtered_data, output_file_format='excel'):\n",
    "    # Determine the output directory based on the file format\n",
    "    if output_file_format == 'excel':\n",
    "        output_directory = output_excel_directory\n",
    "        output_file_path = output_excel_file_path_timeseries\n",
    "    else:\n",
    "        output_directory = output_csv_directory\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    if output_file_format == 'excel':\n",
    "        # Check if the Excel file exists to decide on the mode\n",
    "        mode = 'a' if os.path.exists(output_file_path) else 'w'\n",
    "\n",
    "        # Open the ExcelWriter\n",
    "        with pd.ExcelWriter(output_file_path, engine='openpyxl', mode=mode) as writer:\n",
    "            for sheet_name, df in filtered_data.items():\n",
    "                if mode == 'a' and sheet_name in writer.book.sheetnames:\n",
    "                    del writer.book[sheet_name]\n",
    "                df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "    elif output_file_format == 'csv':\n",
    "        for file_name, df in filtered_data.items():\n",
    "            output_file_path = os.path.join(output_directory, f\"{file_name}.csv\")\n",
    "            df.to_csv(output_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6969f79a-8e2f-4a6e-8728-916b67c7dc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and filter time series data\n",
    "filtered_timeseries_data = read_filter_timeseries(timeseries_directory, unique_values_concatenated)\n",
    "\n",
    "# Output the processed data\n",
    "output_timeseries_data(filtered_timeseries_data, output_file_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec61526-cc83-4aa4-ad24-37934ab92feb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
