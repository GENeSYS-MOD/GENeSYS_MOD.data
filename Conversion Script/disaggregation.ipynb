{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import yaml\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the configuration from the YAML file\n",
    "def load_config(config_file):\n",
    "    with open(config_file, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process unchanged files\n",
    "def process_files(base_dir):\n",
    "    for root, dirs, files in os.walk(base_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('csv'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                df = pd.read_csv(file_path)\n",
    "                yield df, file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_unchanged_files(config, main_folder, output_path):\n",
    "    # Process unchanged files\n",
    "    unchanged_files = config['parameters']['unchanged_files'] #list of files\n",
    "    # Load each CSV file from the main folder\n",
    "    for df, file_path in process_files(main_folder):\n",
    "        param_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "        if param_name in unchanged_files:\n",
    "            #print(f\"Skipping {param_name} as it is in the no_change_parameters list.\")\n",
    "            file_path = os.path.join(output_path, f'{param_name}.csv')\n",
    "            df.to_csv(file_path)\n",
    "            \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_changed_files(config, main_folder, output_path):\n",
    "    changed_files = config['parameters']['changed_files'] # list of file with parameters\n",
    "    \n",
    "    #Load each CSV file from the main folder\n",
    "    for df, file_path in process_files(main_folder):\n",
    "        #combined_new_regions = pd.DataFrame()\n",
    "        param_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "        # Construct the full file path\n",
    "        file_path = os.path.join(output_path, f'{param_name}.csv')\n",
    "        for file in changed_files:\n",
    "            if param_name == file['file_name']:\n",
    "                method = file['method']\n",
    "                index_count = file['index_count']\n",
    "                regions_to_split = file['regions_to_split']\n",
    "                new_regions = file['new_regions']\n",
    "                split_ratio = file['split_ratio']\n",
    "                print(param_name, method, index_count, regions_to_split, new_regions, split_ratio)\n",
    "                df.set_index(df.columns[:index_count].tolist(), inplace=True)\n",
    "                \n",
    "                # Process regions to split\n",
    "                for region in regions_to_split:\n",
    "                    if region in df.index.get_level_values('Region'):\n",
    "                        print(\"True\")\n",
    "                        # Filter rows for the region to split\n",
    "                        region_data = df.loc[region]\n",
    "                        # Create new DataFrame for the split regions\n",
    "                        new_region_dfs = []\n",
    "                        for new_region, ratio in split_ratio.items():\n",
    "                            new_data = region_data.copy()\n",
    "                            new_data['Value'] *= ratio\n",
    "                            new_data['Region'] = new_region\n",
    "                            new_data.reset_index(inplace=True)\n",
    "                            new_region_dfs.append(new_data) \n",
    "                        combined_new_regions = pd.concat(new_region_dfs, ignore_index=True)\n",
    "                        # Add the new regions back to the original DataFrame\n",
    "                        df.reset_index(inplace=True)  # Temporarily reset the index\n",
    "                        df = pd.concat([df, combined_new_regions], ignore_index=True)\n",
    "\n",
    "                        # Reapply multi-index  \n",
    "                        df.set_index(df.columns[:index_count].tolist(), inplace=True)\n",
    "\n",
    "                        # Save the DataFrame to the constructed path\n",
    "                        df.to_csv(file_path)   \n",
    "\n",
    "                    else:\n",
    "                        print(\"do not have region in dataset\")\n",
    "                        df.to_csv(file_path)    \n",
    "\n",
    "                        combined_new_regions = pd.concat(new_region_dfs, ignore_index=True)            "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "def trade_files_(config, main_folder, output_path):\n",
    "    trade_files = config['parameters']['trade_files']\n",
    "    # Load each CSV file from the main folder\n",
    "    for df, file_path in process_files(main_folder):\n",
    "        param_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "\n",
    "        # Construct the full file path\n",
    "        file_path = os.path.join(output_path, f'{param_name}.csv')\n",
    "\n",
    "        for file in trade_files:            \n",
    "            if param_name == file['file_name']:\n",
    "                method = file['method']\n",
    "                index_count = file['index_count']\n",
    "                regions_to_split = file['regions_to_split']\n",
    "                new_regions = file['new_regions']\n",
    "                split_ratio = file['split_ratio']\n",
    "                connection_values = file['value']\n",
    "                print(param_name, method, index_count, regions_to_split, new_regions, split_ratio)\n",
    "                df.set_index(df.columns[:index_count].tolist(), inplace=True)    \n",
    "\n",
    "                new_rows = []\n",
    "                trade_rows = []\n",
    "\n",
    "                # Process regions to split\n",
    "                for region in regions_to_split:\n",
    "                    if region in df.index.get_level_values('Region'):\n",
    "                        print(\"True\")\n",
    "                        # Filter rows for the region to split\n",
    "                        region_data = df.loc[region]\n",
    "                        # Create new DataFrame for the split regions\n",
    "                        new_region_dfs = []\n",
    "                        for new_region, ratio in split_ratio.items():\n",
    "                            new_data = region_data.copy()\n",
    "                            new_data['Value'] *= ratio\n",
    "                            new_data['Region'] = new_region\n",
    "                            new_data.reset_index()\n",
    "                            new_region_dfs.append(new_data) \n",
    "                        combined_new_regions = pd.concat(new_region_dfs, ignore_index=True)\n",
    "\n",
    "                        #Logic to add connection between newly created regions\n",
    "                        years = df['Year'].unique()\n",
    "                        for year in years:\n",
    "                            for connection, value in connection_values.items():\n",
    "                                source_region, target_region = connection.split(' - ')\n",
    "                                trade_row = {\n",
    "                                    'Region': source_region,\n",
    "                                    'Region2': target_region,\n",
    "                                    'Fuel': 'Power',  # Default fuel type for trade capacities\n",
    "                                    'Year': year,  # Default year, can be changed\n",
    "                                    'Value': value,\n",
    "                                    'Unit': 'MW'  # Default unit, can be updated\n",
    "                                }\n",
    "                                trade_rows.append(trade_row)\n",
    "                                \n",
    "                        # Combine everything\n",
    "                        #new_df = pd.DataFrame(new_rows)\n",
    "                        trade_df = pd.DataFrame(trade_rows)\n",
    "                        final_df = pd.concat([df, combined_new_regions, trade_df], ignore_index=True)\n",
    "                        # Reapply multi-index  \n",
    "                        final_df.set_index(df.columns[:index_count].tolist(), inplace=True)\n",
    "\n",
    "                        # Save the DataFrame to the constructed path\n",
    "                        final_df.to_csv(file_path) \n",
    "                    else:\n",
    "                        print(\"do not have region in dataset\")\n",
    "                        df.to_csv(file_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Par_TotalAnnualMaxActivity copy 3 ['NO'] ['NO1', 'NO2'] {'NO1': 1.0, 'NO2': 1.0}\n",
      "True\n",
      "Par_REMinProductionTarget copy 3 ['NO'] ['NO1', 'NO2'] {'NO1': 1.0, 'NO2': 1.0}\n",
      "do not have region in dataset\n",
      "Par_ReserveMargin copy 2 ['NO'] ['NO1', 'NO2'] {'NO1': 1.0, 'NO2': 1.0}\n",
      "True\n",
      "Par_ModalSplitByFuel copy 4 ['NO'] ['NO1', 'NO2'] {'NO1': 1.0, 'NO2': 1.0}\n",
      "True\n",
      "Par_CapitalCost copy 3 ['NO'] ['NO1', 'NO2'] {'NO1': 1.0, 'NO2': 1.0}\n",
      "do not have region in dataset\n",
      "Par_TechnologyDiscountRate copy 2 ['NO'] ['NO1', 'NO2'] {'NO1': 1.0, 'NO2': 1.0}\n",
      "True\n",
      "Par_ResidualCapacity copy 3 ['NO'] ['NO1', 'NO2'] {'NO1': 1.0, 'NO2': 1.0}\n",
      "True\n",
      "Par_GrowthRateTradeCapacity copy 4 ['NO'] ['NO1', 'NO2'] {'NO1': 1.0, 'NO2': 1.0}\n",
      "True\n",
      "Par_TradeCapacity copy 4 ['NO'] ['NO1', 'NO2'] {'NO1': 1.0, 'NO2': 1.0}\n",
      "True\n",
      "Par_RegionalAnnualEmissionLimit copy 3 ['NO'] ['NO1', 'NO2'] {'NO1': 1.0, 'NO2': 1.0}\n",
      "True\n",
      "Par_AnnualExogenousEmission copy 3 ['NO'] ['NO1', 'NO2'] {'NO1': 1.0, 'NO2': 1.0}\n",
      "do not have region in dataset\n",
      "Par_RegionalModelPeriodEmissionLimit copy 2 ['NO'] ['NO1', 'NO2'] {'NO1': 1.0, 'NO2': 1.0}\n",
      "True\n",
      "Par_BaseYearProduction copy 3 ['NO'] ['NO1', 'NO2'] {'NO1': 1.0, 'NO2': 1.0}\n",
      "do not have region in dataset\n",
      "Par_TradeCapacityGrowthCosts copy 3 ['NO'] ['NO1', 'NO2'] {'NO1': 1.0, 'NO2': 1.0}\n",
      "True\n",
      "Par_SpecifiedAnnualDemand copy 3 ['NO'] ['NO1', 'NO2'] {'NO1': 1.0, 'NO2': 1.0}\n",
      "True\n",
      "Par_TotalAnnualMaxCapacity copy 3 ['NO'] ['NO1', 'NO2'] {'NO1': 1.0, 'NO2': 1.0}\n",
      "True\n",
      "Par_RegionalBaseYearProduction copy 4 ['NO'] ['NO1', 'NO2'] {'NO1': 1.0, 'NO2': 1.0}\n",
      "True\n",
      "Par_OutputActivityRatio copy 4 ['NO'] ['NO1', 'NO2'] {'NO1': 1.0, 'NO2': 1.0}\n",
      "do not have region in dataset\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "base_dir = \"/Users/shwetat/Projects/Genesys-mod_data_repo/GENeSYS_MOD.data/Data/Parameters\"\n",
    "output_dir = \"/Users/shwetat/Projects/Genesys-mod_data_repo/GENeSYS_MOD.data/DataNew\"\n",
    "config = load_config(\"config_disaggregation.yaml\")\n",
    "process_unchanged_files(config, base_dir, output_dir)\n",
    "process_changed_files(config, base_dir, output_dir)\n",
    "#trade_files_(config, base_dir, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                \"\"\" #Combine the new region DataFrames\n",
    "                combined_new_regions = pd.concat(new_region_dfs, ignore_index=True)\n",
    "\n",
    "                # Add the new regions back to the original DataFrame\n",
    "                df.reset_index(inplace=True)  # Temporarily reset the index\n",
    "                df = pd.concat([df, combined_new_regions], ignore_index=True)\n",
    "\n",
    "                # Reapply multi-index  \n",
    "                df.set_index(df.columns[:index_count].tolist(), inplace=True)\n",
    "\n",
    "                # Construct the full file path\n",
    "                file_path = os.path.join(output_path, f'{param_name}.csv')\n",
    "\n",
    "                # Save the DataFrame to the constructed path\n",
    "                df.to_csv(file_path) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example function to update DataFrame with new regions\n",
    "def add_new_regions(df, region_to_split, new_regions):\n",
    "    \"\"\"\n",
    "    Function to add new regions to the DataFrame by copying values of a given region.\n",
    "    \n",
    "    Args:\n",
    "    - df (pd.DataFrame): Input DataFrame\n",
    "    - region_to_split (str): The region whose data needs to be copied\n",
    "    - new_regions (list): List of new regions to add\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: Updated DataFrame with new regions\n",
    "    \"\"\"\n",
    "    # Filter rows matching the region_to_split\n",
    "    rows_to_copy = df[df['Region'] == region_to_split]\n",
    "    \n",
    "    # Create new rows by duplicating rows_to_copy for each new region\n",
    "    for new_region in new_regions:\n",
    "        new_rows = rows_to_copy.copy()\n",
    "        new_rows['Region'] = new_region\n",
    "        df = pd.concat([df, new_rows], ignore_index=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Load DataFrame (replace with your actual file path)\n",
    "    file_path = \"/Users/shwetat/Projects/Genesys-mod_data_repo/GENeSYS_MOD.data/Data/Parameters/Par_GeneralDiscountRate/Par_GeneralDiscountRate.csv\"  # Change this to the actual file path\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Parameters for the regions to copy and add\n",
    "    region_to_split = \"NO\"  # Region to split (source region)\n",
    "    new_regions = [\"NO1\", \"NO2\"]  # New regions to create\n",
    "    \n",
    "    # Update DataFrame\n",
    "    updated_df = add_new_regions(df, region_to_split, new_regions)\n",
    "    \n",
    "    # Save updated DataFrame\n",
    "    #updated_file_path = \"updated_\" + file_path\n",
    "    updated_df.to_csv('generaldiscountrate.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_indexed(config, main_folder, output_path):\n",
    "    single_index_file = config['parameters']['changed_files1'] # list of file with parameters\n",
    "    \n",
    "    #Load each CSV file from the main folder\n",
    "    for df, file_path in process_files(main_folder):\n",
    "        #combined_new_regions = pd.DataFrame()\n",
    "        param_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "       \n",
    "        # Construct the full file path\n",
    "        file_path = os.path.join(output_path, f'{param_name}.csv')\n",
    "        for file in single_index_file:\n",
    "            if param_name == file['file_name']:\n",
    "                print(param_name)\n",
    "                method = file['method']\n",
    "                \n",
    "                regions_to_split = file['regions_to_split']\n",
    "                new_regions = file['new_regions']\n",
    "                split_ratio = file['split_ratio']\n",
    "                df = df.set_index(['Region'])\n",
    "                #print(df.head())\n",
    "                \n",
    "                \n",
    "                # Process regions to split\n",
    "                for region in regions_to_split:\n",
    "                    if region in df.index.get_level_values('Region'):\n",
    "                        print(df.head())\n",
    "                        # Filter rows for the region to split\n",
    "                    region_data = df.loc[region]\n",
    "                    print(region_data.head())\n",
    "                    # Create new DataFrame for the split regions\n",
    "                    new_region_dfs = []\n",
    "                    for new_region, ratio in split_ratio.items():\n",
    "                        #print(new_region, ratio)\n",
    "                        new_data = region_data.copy()\n",
    "\n",
    "                        new_data['Value'] *= ratio\n",
    "                        new_data['Region'] = new_region\n",
    "                        new_region_dfs.append(new_data) \n",
    "                        combined_new_regions = pd.concat(new_region_dfs, ignore_index=True)\n",
    "                        # Add the new regions back to the original DataFrame\n",
    "                        \n",
    "                        df = pd.concat([df, combined_new_regions], ignore_index=True)\n",
    "\n",
    "                        # Reapply multi-index  \n",
    "                        \n",
    "\n",
    "                        # Save the DataFrame to the constructed path\n",
    "                        df.to_csv(file_path)   \n",
    "\n",
    "                    else:\n",
    "                        print(\"do not have region in dataset\")\n",
    "                        df.to_csv(file_path)    \n",
    "\n",
    "                        combined_new_regions = pd.concat(new_region_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Par_GeneralDiscountRate\n",
      "        Value  Unnamed: 2     Unit      Source  Updated at  \\\n",
      "Region                                                       \n",
      "AT       0.05         NaN  Percent  Assumption  08.09.2023   \n",
      "BE       0.05         NaN  Percent  Assumption  08.09.2023   \n",
      "BG       0.05         NaN  Percent  Assumption  08.09.2023   \n",
      "CH       0.05         NaN  Percent  Assumption  08.09.2023   \n",
      "CZ       0.05         NaN  Percent  Assumption  08.09.2023   \n",
      "\n",
      "                                      Updated by  \n",
      "Region                                            \n",
      "AT      Konstantin Löffler <kl@wip.tu-berlin.de>  \n",
      "BE      Konstantin Löffler <kl@wip.tu-berlin.de>  \n",
      "BG      Konstantin Löffler <kl@wip.tu-berlin.de>  \n",
      "CH      Konstantin Löffler <kl@wip.tu-berlin.de>  \n",
      "CZ      Konstantin Löffler <kl@wip.tu-berlin.de>  \n",
      "Value               0.05\n",
      "Unnamed: 2           NaN\n",
      "Unit             Percent\n",
      "Source        Assumption\n",
      "Updated at    08.09.2023\n",
      "Name: NO, dtype: object\n",
      "do not have region in dataset\n"
     ]
    }
   ],
   "source": [
    "single_indexed(config, base_dir, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "havnett",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
