{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6672c8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206459e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet_no_mod = ['Par_TradeRoute',\n",
    "             'Par_TradeCosts',\n",
    "             'Par_TradeCapacity',\n",
    "             'Par_TradeCapacityGrowthCosts',\n",
    "             'Par_GrowthRateTradeCapacity',\n",
    "             'Par_RegionalAnnualEmissionLimit',\n",
    "             'Par_EmissionsPenalty',\n",
    "             'Par_SpecifiedAnnualDemand',\n",
    "             'Par_ReserveMargin',\n",
    "             'Par_ResidualCapacity',\n",
    "             'Par_AvailabilityFactor',\n",
    "             'Par_TotalAnnualMaxActivity',\n",
    "             'Par_TotalAnnualMaxCapacity',\n",
    "             'Par_ModelPeriodActivityMaxLimit',\n",
    "             'Par_RegionalCCSLimit',\n",
    "             'Par_RegionalBaseYearProduction',\n",
    "             'Par_ModalSplitByFuel',\n",
    "             'Sets',\n",
    "             'Par_DaySplit',\n",
    "             'Par_YearSplit',\n",
    "             'Par_TagTechnologyToSector',\n",
    "             'Par_CapacityToActivityUnit',\n",
    "             'Par_Conversionls',\n",
    "             'Par_Conversionld',\n",
    "             'Par_Conversionlh',\n",
    "             'Par_AnnualExogenousEmission',\n",
    "             'Par_AnnualEmissionLimit',\n",
    "             'Par_AnnualSectoralEmissionLimit',\n",
    "             'Par_EmissionActivityRatio',\n",
    "             'Par_EmissionPenaltyTagTech',\n",
    "             'Par_EmissionContentPerFuel',\n",
    "             'Par_ReserveMarginTagFuel',\n",
    "             'Par_ReserveMarginTagTechnology',\n",
    "             'Par_InputActivityRatio',\n",
    "             'Par_OutputActivityRatio',\n",
    "             'Par_CapitalCost',\n",
    "             'Par_VariableCost',\n",
    "             'Par_FixedCost',\n",
    "             'Par_CapacityFactor',\n",
    "             'Par_OperationalLife',\n",
    "             'Par_TotalAnnualMinActivity',\n",
    "             'Par_TotalAnnualMinCapacity',\n",
    "             'Par_TagTechnologyToModalType',\n",
    "             'Par_BaseYearProduction',\n",
    "             'Par_TechnologyToStorage',\n",
    "             'Par_TechnologyFromStorage',\n",
    "             'Par_StorageLevelStart',\n",
    "             'Par_StorageMaxChargeRate',\n",
    "             'Par_StorageMaxDischargeRate',\n",
    "             'Par_MinStorageCharge',\n",
    "             'Par_OperationalLifeStorage',\n",
    "             'Par_CapitalCostStorage',\n",
    "             'Par_ResidualStorageCapacity']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea37739",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataRaw:\n",
    "\n",
    "    def __init__(self, filepath, outputpath):\n",
    "        self.filepath = filepath\n",
    "        self.outputpath = outputpath\n",
    "        self.name = None\n",
    "        self.dic_sheets = {}\n",
    "        self.df = pd.DataFrame\n",
    "        self.process_excel()\n",
    "        \n",
    "    def process_excel(self):\n",
    "\n",
    "            \"\"\"\n",
    "            Iterate through Excel sheets and make dictionary\n",
    "            \"\"\"\n",
    "            for sheet in sheet_no_mod:\n",
    "                self.name = sheet\n",
    "                print(self.name)\n",
    "               \n",
    "                df_first = pd.read_excel(self.filepath,\n",
    "                                        sheet_name=sheet,\n",
    "                                        engine='openpyxl', header=None)\n",
    "\n",
    "                r = 5\n",
    "                df_final = df_first.iloc[:r] #shweta\n",
    "                df_final = df_final.apply(lambda x: x.apply(lambda y: str(y).replace('.0', '')))\n",
    "                df_raw = df_first.iloc[r:]\n",
    "                df_raw = self.remove_blank_rows(df_raw)\n",
    "                df_raw = df_raw.apply(lambda x: x.apply(lambda y: str(y).replace('.', ',')))\n",
    "                df = pd.concat([df_final, df_raw], axis=0)\n",
    "                df.replace('nan', '', inplace=True) # remove 'nan' with nothing\n",
    "                df.to_csv((self.outputpath + f'{self.name}.csv'), sep=';', index=False, header=False)\n",
    "\n",
    "                \n",
    "    def remove_blank_rows(self, df):\n",
    "        \"\"\"\n",
    "        :param df pandas dataframe from excel sheet\n",
    "        :return: dataframe with removed nan values\n",
    "        \"\"\"\n",
    "        # remove blank rows\n",
    "        try:\n",
    "            df = df.loc[0:df[df.isnull().all(axis=1) == True].index.tolist()[0] - 1]\n",
    "        except IndexError:\n",
    "            pass\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7bfc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENAME = \"../../NO_disagg_GradualDevelopment_oE_v492.xlsx\"\n",
    "\n",
    "FILE_EXTENSION = \"csv\"\n",
    "\n",
    "PATH_TO_CSV = \"../../csvs/\"\n",
    "\n",
    "df = DataRaw(filepath=FILENAME,\n",
    "                  outputpath = PATH_TO_CSV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "249d6e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Method to read the csv files from the folder one by one\n",
    "\"\"\"\n",
    "class Process_csv:\n",
    "    def __init__(self, folderpath):\n",
    "        self.folderpath = folderpath\n",
    "        self.name = None\n",
    "        self.df = pd.DataFrame\n",
    "        #self.filter_region = filter_region\n",
    "        self.read_csvs()\n",
    "        \n",
    "    def read_csvs(self): \n",
    "        # Loop through all the files in the folder\n",
    "        for filename in os.listdir(self.folderpath):\n",
    "            if filename.endswith('.csv'):  # Ensure the file is a CSV file\n",
    "                file_path = os.path.join(self.folderpath, filename)\n",
    "                if filename == 'Par_ResidualCapacity.csv':\n",
    "                    # Read the CSV file and create a DataFrame\n",
    "                    self.df = pd.read_csv(file_path,  sep=';', skiprows=4)\n",
    "                    if self.df[(self.df.columns[0] == 'AT')]:\n",
    "                        filtered_df = self.df\n",
    "                    \n",
    "                    #filtered_df = self.df[self.df.columns[0] == 'AL']\n",
    "                    print(self.df.shape)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a06739f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/envs/havnett/lib/python3.10/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3628\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3629\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3630\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/havnett/lib/python3.10/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/havnett/lib/python3.10/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: False",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ww/34wy9_qd5f5_r6d15cv5m8rh0000gn/T/ipykernel_96115/1670536521.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mFOLDER_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/Users/shwetat/Projects/Ocean_grid/Havnett_repo/csvs/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#filter_region = ['AT', 'SE']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mProcess_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolderpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFOLDER_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/ww/34wy9_qd5f5_r6d15cv5m8rh0000gn/T/ipykernel_96115/379641076.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, folderpath)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m#self.filter_region = filter_region\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csvs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread_csvs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/ww/34wy9_qd5f5_r6d15cv5m8rh0000gn/T/ipykernel_96115/379641076.py\u001b[0m in \u001b[0;36mread_csvs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     18\u001b[0m                     \u001b[0;31m# Read the CSV file and create a DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m';'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskiprows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'AT'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m                         \u001b[0mfiltered_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/havnett/lib/python3.10/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3503\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3504\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3505\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3506\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3507\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/havnett/lib/python3.10/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3629\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3630\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3631\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3632\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3633\u001b[0m                 \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: False"
     ]
    }
   ],
   "source": [
    "FOLDER_PATH = '/Users/shwetat/Projects/Ocean_grid/Havnett_repo/csvs/'\n",
    "#filter_region = ['AT', 'SE']\n",
    "Process_csv(folderpath=FOLDER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eba9a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to combine csvs into 1 xlsx file\n",
    "def get_file_list(path_to_csvs, file_extension):\n",
    "    \"\"\"\n",
    "    Calls function to get list of all csv files\n",
    "    :para path_to_csv: path to csv folder\n",
    "    :para file_extension: csv in this case\n",
    "    :return list of csv file names\n",
    "    \"\"\"\n",
    "    filenames = []\n",
    "    for file in os.listdir(path_to_csvs):\n",
    "        if file.endswith(file_extension):\n",
    "            filenames.append(file)\n",
    "    return filenames\n",
    "    \n",
    "def combine_xlsx(path_to_csvs, all_filenames, output_file):  \n",
    "    \"\"\"\n",
    "    Calls function to combine csv files\n",
    "    :para path_to_csv: path to csv folder\n",
    "    :para all_filenames: list of csv files in the folder\n",
    "    :para output_file: name and extension of output file\n",
    "    :return list of csv file names\n",
    "    \"\"\"\n",
    "    print(path_to_csvs)\n",
    "    writer = pd.ExcelWriter(path_to_csvs + output_file) \n",
    "    for csvfilename in all_filenames: \n",
    "        print(\"Loading \"+ csvfilename)\n",
    "        df = pd.read_csv(path_to_csvs + csvfilename, sep=';', encoding='utf-8')\n",
    "        # df_final =  df1.iloc[:4, :1]\n",
    "        # df_t = df1[4:]\n",
    "        # df_t = df_t.apply(lambda x: x.apply(lambda y: str(y).replace('.', ',')))\n",
    "        # df = pd.concat([df_final, df_t], axis=0)\n",
    "        for j in range(len(df.columns)):\n",
    "            if j == 0:\n",
    "                print('do nothing')\n",
    "            else:\n",
    "                old = df.columns[j]\n",
    "                new = ''\n",
    "                df = df.rename(columns = {old:new})\n",
    "        df.to_excel(writer, sheet_name=csvfilename[:-4], index=False, header=True)\n",
    "        \n",
    "        print(\"done\")\n",
    "        #break\n",
    "    writer.save()\n",
    "    print(\"task completed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
