{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43d3defb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "import math\n",
    "\n",
    "# Define the file to exclude\n",
    "excluded_file = 'Sets.csv'\n",
    "\n",
    "# Define the directory where you want to save the filtered CSV files\n",
    "output_csv_directory = 'output_csv'  # Change this to the directory where you want to save the filtered CSV files\n",
    "\n",
    "# Create the output directory for CSV files if it doesn't exist\n",
    "os.makedirs(output_csv_directory, exist_ok=True)\n",
    "\n",
    "# Define the directory where you want to save the Excel file\n",
    "output_excel_directory = 'output_excel'  # Change this to the directory where you want to save the Excel file\n",
    "\n",
    "# Create the output directory for the Excel file if it doesn't exist\n",
    "os.makedirs(output_excel_directory, exist_ok=True)\n",
    "\n",
    "# Specify the Excel file path\n",
    "excel_file_path = 'GENeSYS-MOD_User_Input_Settings_v04_kh_08-09-2023.xlsx'  # Replace with the path to your Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d170de21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the Excel file\n",
    "xls = pd.ExcelFile(excel_file_path, engine='openpyxl')\n",
    "\n",
    "# Get the list of sheet names in the Excel file\n",
    "sheets_to_read = xls.sheet_names\n",
    "\n",
    "# Initialize an empty dictionary to store DataFrames\n",
    "data_frames = {}\n",
    "filtered_df = {}\n",
    "unique_values = {}\n",
    "\n",
    "unique_values_concatenated = pd.DataFrame()\n",
    "column_list = []\n",
    "\n",
    "# Read sheets and store them in the dictionary\n",
    "for sheet_name in sheets_to_read:\n",
    "    data_frames = xls.parse(sheet_name)\n",
    "\n",
    "    filtered_df= data_frames[data_frames.iloc[:, 1] == 1] # Assuming the second column is indexed at 1 (0-based index)\n",
    "\n",
    "    column_list.append(filtered_df.columns[0]) # collect column header for each set sheet\n",
    "   \n",
    "    unique_values= pd.DataFrame(filtered_df.iloc[:, 0].unique())  # Assuming the first column is indexed at 0 (0-based index)\n",
    "    unique_values_parameter = pd.DataFrame(unique_values)\n",
    "    \n",
    "    unique_values_concatenated = pd.concat([unique_values_concatenated, unique_values], axis=1)\n",
    "\n",
    "# Need to put header to the dataframe\n",
    "unique_values_concatenated.columns = column_list\n",
    "\n",
    "# Create a CSV file containing unique values\n",
    "unique_values_csv_file_path = os.path.join(output_csv_directory, 'Sets.csv')\n",
    "unique_values_concatenated.to_csv(unique_values_csv_file_path, index=False, decimal='.') \n",
    "\n",
    "if \"Region\" in unique_values_concatenated.columns:\n",
    "    unique_values_concatenated[\"Region2\"] = unique_values_concatenated[\"Region\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff37fca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty dictionary to store DataFrames\n",
    "data_frames = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a45607b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Par_AnnualEmissionLimit\\Par_AnnualEmissionLimit.csv\n",
      "Empty DataFrame\n",
      "Columns: [Emission, Year, Value]\n",
      "Index: []\n",
      "Year Emission   2018   2020   2025   2030   2040   2045   2050\n",
      "0         CO2  99999  99999  99999  99999  99999  99999  99999\n",
      "Par_AnnualExogenousEmission\\Par_AnnualExogenousEmission.csv\n",
      "Empty DataFrame\n",
      "Columns: [Region, Technology, Year, Value]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [Region, Technology]\n",
      "Index: []\n",
      "Par_AnnualSectoralEmissionLimit\\Par_AnnualSectoralEmissionLimit.csv\n",
      "Empty DataFrame\n",
      "Columns: [Emission, Sector, Year, Value]\n",
      "Index: []\n",
      "Year Emission     Sector    2018    2020    2025    2030    2040    2045  \\\n",
      "0         CO2  Buildings  999999  999999  999999  999999  999999  999999   \n",
      "1         CO2        CHP  999999  999999  999999  999999  999999  999999   \n",
      "2         CO2      Power  999999  999999  999999  999999  999999  999999   \n",
      "3         CO2  Resources  999999  999999  999999  999999  999999  999999   \n",
      "4         CO2   Storages  999999  999999  999999  999999  999999  999999   \n",
      "\n",
      "Year    2050  \n",
      "0     999999  \n",
      "1     999999  \n",
      "2     999999  \n",
      "3     999999  \n",
      "4     999999  \n",
      "Par_AvailabilityFactor\\Par_AvailabilityFactor.csv\n",
      "Empty DataFrame\n",
      "Columns: [Region, Technology, Year, Value]\n",
      "Index: []\n",
      "Year Region       Technology      2018      2020      2025      2030  \\\n",
      "0        AT  RES_Hydro_Large  0.284003  0.284003  0.284003  0.284003   \n",
      "1        BE  RES_Hydro_Large  0.096725  0.096725  0.096725  0.096725   \n",
      "2        BG  RES_Hydro_Large  0.134024  0.134024  0.134024  0.134024   \n",
      "3        CH  RES_Hydro_Large  0.293486  0.293486  0.293486  0.293486   \n",
      "4        CZ  RES_Hydro_Large  0.127495  0.127495  0.127495  0.127495   \n",
      "\n",
      "Year      2040      2045      2050  \n",
      "0     0.284003  0.284003  0.284003  \n",
      "1     0.096725  0.096725  0.096725  \n",
      "2     0.134024  0.134024  0.134024  \n",
      "3     0.293486  0.293486  0.293486  \n",
      "4     0.127495  0.127495  0.127495  \n",
      "Par_BaseYearProduction\\Par_BaseYearProduction.csv\n",
      "Empty DataFrame\n",
      "Columns: [Region, Technology, Year, Value]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [Region, Technology]\n",
      "Index: []\n",
      "Par_CapacityFactor\\Par_ResidualCapacity.csv\n",
      "Empty DataFrame\n",
      "Columns: [Region, Technology, Year, Value]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [Region, Technology]\n",
      "Index: []\n",
      "Par_CapitalCost\\Par_CapitalCost.csv\n",
      "Empty DataFrame\n",
      "Columns: [Region, Technology, Year, Value]\n",
      "Index: []\n",
      "Year Region      Technology       2018       2020       2025       2030  \\\n",
      "0        DE    FRT_Road_LNG   6.122449   6.020825   5.920033   5.820075   \n",
      "1        DE    FRT_Ship_LNG  10.303618   9.964305   9.628817   9.297151   \n",
      "2        DE  HLI_Convert_DH   1.000000   1.000000   1.000000   1.000000   \n",
      "3        DE  HLR_Convert_DH   1.000000   1.000000   1.000000   1.000000   \n",
      "4        DE   PSNG_Road_LNG  48.000000  43.688146  39.433399  35.235761   \n",
      "\n",
      "Year       2040       2045       2050  \n",
      "0      5.622657   5.525198   5.428571  \n",
      "1      8.645290   8.325094   8.008721  \n",
      "2      1.000000   1.000000   1.000000  \n",
      "3      1.000000   1.000000   1.000000  \n",
      "4     27.011808  22.985493  19.016286  \n",
      "Par_CapitalCostStorage\\Par_CapitalCostStorage.csv\n",
      "Empty DataFrame\n",
      "Columns: [Region, Storage, Year, Value]\n",
      "Index: []\n",
      "Year Region           Storage  2018\n",
      "0        DE  S_Battery_Li-Ion     0\n",
      "1        DE   S_Battery_Redox     0\n",
      "2        DE            S_CAES     0\n",
      "3        DE     S_Gas_Methane     0\n",
      "4        DE        S_Heat_HLI     0\n",
      "Par_CommissionedTradeCapacity\\Par_CommissionedTradeCapacity.csv\n",
      "Empty DataFrame\n",
      "Columns: [Region, Fuel, Year, Value]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [Region, Fuel]\n",
      "Index: []\n",
      "Par_EmissionActivityRatio\\Par_EmissionActivityRatio.csv\n",
      "Empty DataFrame\n",
      "Columns: [Region, Technology, Mode_of_operation, Emission, Year, Value]\n",
      "Index: []\n",
      "Year Region     Technology  Mode_of_operation Emission  2018  2020  2025  \\\n",
      "0        DE  FRT_Rail_Conv                  1      CO2   1.0   1.0   1.0   \n",
      "1        DE   FRT_Road_ICE                  1      CO2   1.0   1.0   1.0   \n",
      "2        DE   FRT_Road_LNG                  1      CO2   1.0   1.0   1.0   \n",
      "3        DE  FRT_Road_PHEV                  1      CO2   1.0   1.0   1.0   \n",
      "4        DE  FRT_Ship_Conv                  1      CO2   1.0   1.0   1.0   \n",
      "\n",
      "Year  2030  2040  2045  2050  \n",
      "0      1.0   1.0   1.0   1.0  \n",
      "1      1.0   1.0   1.0   1.0  \n",
      "2      1.0   1.0   1.0   1.0  \n",
      "3      1.0   1.0   1.0   1.0  \n",
      "4      1.0   1.0   1.0   1.0  \n",
      "Par_EmissionsPenalty\\Par_EmissionsPenalty.csv\n",
      "Empty DataFrame\n",
      "Columns: [Region, Emission, Year, Value]\n",
      "Index: []\n",
      "Year Region Emission   2018  2020   2025        2030         2040  \\\n",
      "0        AT      CO2  15.06  30.0  325.0  577.857143  1184.714286   \n",
      "1        BE      CO2  15.06  30.0  325.0  577.857143  1184.714286   \n",
      "2        BG      CO2  15.06  30.0  325.0  577.857143  1184.714286   \n",
      "3        CH      CO2  15.06  30.0  325.0  577.857143  1184.714286   \n",
      "4        CZ      CO2  15.06  30.0  325.0  577.857143  1184.714286   \n",
      "\n",
      "Year         2045    2050  \n",
      "0     1492.540373  1800.0  \n",
      "1     1492.540373  1800.0  \n",
      "2     1492.540373  1800.0  \n",
      "3     1492.540373  1800.0  \n",
      "4     1492.540373  1800.0  \n",
      "Par_FixedCost\\Par_FixedCost.csv\n",
      "Empty DataFrame\n",
      "Columns: [Region, Technology, Year, Value]\n",
      "Index: []\n",
      "Year Region      Technology       2018       2020       2025       2030  \\\n",
      "0        DE    FRT_Road_LNG   6.122449   6.064140   6.005831   5.947522   \n",
      "1        DE    FRT_Ship_LNG  10.303618  10.035991   9.768365   9.500738   \n",
      "2        DE  HLI_Convert_DH   1.000000   1.000000   1.000000   1.000000   \n",
      "3        DE  HLR_Convert_DH   1.000000   1.000000   1.000000   1.000000   \n",
      "4        DE   PSNG_Road_LNG  48.000000  44.002449  40.004898  36.007347   \n",
      "\n",
      "Year       2040       2045       2050  \n",
      "0      5.830904   5.772595   5.714286  \n",
      "1      8.965485   8.697859   8.430233  \n",
      "2      1.000000   1.000000   1.000000  \n",
      "3      1.000000   1.000000   1.000000  \n",
      "4     28.012245  24.014694  20.017143  \n",
      "Par_GrowthRateTradeCapacity\\Par_GrowthRateTradeCapacity.csv\n",
      "Empty DataFrame\n",
      "Columns: [Region, Region2, Fuel, Year, Value]\n",
      "Index: []\n",
      "Year Region Region2   Fuel  2018\n",
      "0        AT      AT  Power   0.1\n",
      "1        AT      BE  Power   0.1\n",
      "2        AT      BG  Power   0.1\n",
      "3        AT      CH  Power   0.1\n",
      "4        AT      CZ  Power   0.1\n",
      "Par_InputActivityRatio\\Par_InputActivityRatio.csv\n",
      "Empty DataFrame\n",
      "Columns: [Region, Technology, Fuel, Mode_of_operation, Year, Value]\n",
      "Index: []\n",
      "Year Region         Technology       Fuel  Mode_of_operation      2018  \\\n",
      "0        DE      FRT_Rail_Conv        Oil                  1  0.798872   \n",
      "1        DE      FRT_Rail_Conv  Powerfuel                  3  0.798872   \n",
      "2        DE  FRT_Rail_Electric      Power                  1  0.469925   \n",
      "3        DE       FRT_Road_BEV      Power                  1  2.975000   \n",
      "4        DE        FRT_Road_H2         H2                  1  2.985000   \n",
      "\n",
      "Year      2020      2025      2030      2040      2045      2050  \n",
      "0     0.786999  0.772503  0.758121  0.729703  0.715667  0.701745  \n",
      "1     0.786999  0.772503  0.758121  0.729703  0.715667  0.701745  \n",
      "2     0.455237  0.435922  0.417015  0.380423  0.362739  0.345462  \n",
      "3     2.948482  2.909375  2.873408  2.802785  2.767729  2.732679  \n",
      "4     2.959474  2.913089  2.867112  2.776383  2.731630  2.687286  \n",
      "Par_MinStorageCharge\\Par_MinStorageCharge.csv\n",
      "Empty DataFrame\n",
      "Columns: [Region, Storage, Year, Value]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [Region, Storage]\n",
      "Index: []\n",
      "Par_ModalSplitByFuel\\Par_ModalSplitByFuel.csv\n",
      "Empty DataFrame\n",
      "Columns: [Region, Fuel, ModalType, Year, Value]\n",
      "Index: []\n",
      "Year Region              Fuel         ModalType  2018  2020  2025  2030  2040  \\\n",
      "0        AT  Mobility_Freight       MT_FRT_RAIL  0.31  0.31  0.30  0.29  0.27   \n",
      "1        AT  Mobility_Freight  MT_FRT_RAIL_CONV  0.09  0.07  0.00  0.00  0.00   \n",
      "2        AT  Mobility_Freight    MT_FRT_RAIL_RE  0.22  0.21  0.21  0.20  0.19   \n",
      "3        AT  Mobility_Freight       MT_FRT_ROAD  0.66  0.65  0.64  0.61  0.57   \n",
      "4        AT  Mobility_Freight  MT_FRT_ROAD_CONV  0.64  0.61  0.48  0.30  0.00   \n",
      "\n",
      "Year  2045  2050  \n",
      "0     0.26  0.25  \n",
      "1     0.00  0.00  \n",
      "2     0.18  0.17  \n",
      "3     0.55  0.53  \n",
      "4     0.00  0.00  \n",
      "Par_OperationalLifeStorage\\Par_OperationalLifeStorage.csv\n",
      "     Region Storage  Year  Value\n",
      "72      NaN     NaN   NaN    NaN\n",
      "73      NaN     NaN   NaN    NaN\n",
      "74      NaN     NaN   NaN    NaN\n",
      "75      NaN     NaN   NaN    NaN\n",
      "76      NaN     NaN   NaN    NaN\n",
      "...     ...     ...   ...    ...\n",
      "1054    NaN     NaN   NaN    NaN\n",
      "1055    NaN     NaN   NaN    NaN\n",
      "1056    NaN     NaN   NaN    NaN\n",
      "1057    NaN     NaN   NaN    NaN\n",
      "1058    NaN     NaN   NaN    NaN\n",
      "\n",
      "[987 rows x 4 columns]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Index contains duplicate entries, cannot reshape",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 74\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28mprint\u001b[39m(full_duplicates)\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;66;03m# Pivot the DataFrame while setting the specified columns as the index\u001b[39;00m\n\u001b[1;32m---> 74\u001b[0m     pivot_df \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpivot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mYear\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mValue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28mprint\u001b[39m(pivot_df\u001b[38;5;241m.\u001b[39mhead())\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# Formatting steps for both dataframes\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:8567\u001b[0m, in \u001b[0;36mDataFrame.pivot\u001b[1;34m(self, index, columns, values)\u001b[0m\n\u001b[0;32m   8561\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   8562\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_shared_docs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpivot\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m   8563\u001b[0m \u001b[38;5;129m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, allowed_args\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m   8564\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpivot\u001b[39m(\u001b[38;5;28mself\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, values\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[0;32m   8565\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpivot\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pivot\n\u001b[1;32m-> 8567\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpivot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\pivot.py:540\u001b[0m, in \u001b[0;36mpivot\u001b[1;34m(data, index, columns, values)\u001b[0m\n\u001b[0;32m    536\u001b[0m         indexed \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39m_constructor_sliced(data[values]\u001b[38;5;241m.\u001b[39m_values, index\u001b[38;5;241m=\u001b[39mmultiindex)\n\u001b[0;32m    537\u001b[0m \u001b[38;5;66;03m# error: Argument 1 to \"unstack\" of \"DataFrame\" has incompatible type \"Union\u001b[39;00m\n\u001b[0;32m    538\u001b[0m \u001b[38;5;66;03m# [List[Any], ExtensionArray, ndarray[Any, Any], Index, Series]\"; expected\u001b[39;00m\n\u001b[0;32m    539\u001b[0m \u001b[38;5;66;03m# \"Hashable\"\u001b[39;00m\n\u001b[1;32m--> 540\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mindexed\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns_listlike\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py:4455\u001b[0m, in \u001b[0;36mSeries.unstack\u001b[1;34m(self, level, fill_value)\u001b[0m\n\u001b[0;32m   4412\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4413\u001b[0m \u001b[38;5;124;03mUnstack, also known as pivot, Series with MultiIndex to produce DataFrame.\u001b[39;00m\n\u001b[0;32m   4414\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4451\u001b[0m \u001b[38;5;124;03mb    2    4\u001b[39;00m\n\u001b[0;32m   4452\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4453\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m unstack\n\u001b[1;32m-> 4455\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43munstack\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\reshape.py:489\u001b[0m, in \u001b[0;36munstack\u001b[1;34m(obj, level, fill_value)\u001b[0m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_1d_only_ea_dtype(obj\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[0;32m    488\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unstack_extension_series(obj, level, fill_value)\n\u001b[1;32m--> 489\u001b[0m unstacker \u001b[38;5;241m=\u001b[39m \u001b[43m_Unstacker\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconstructor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_constructor_expanddim\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m unstacker\u001b[38;5;241m.\u001b[39mget_result(\n\u001b[0;32m    493\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_values, value_columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, fill_value\u001b[38;5;241m=\u001b[39mfill_value\n\u001b[0;32m    494\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\reshape.py:137\u001b[0m, in \u001b[0;36m_Unstacker.__init__\u001b[1;34m(self, index, level, constructor)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_cells \u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax:\n\u001b[0;32m    130\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    131\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following operation may generate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_cells\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m cells \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    132\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min the resulting pandas object.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    133\u001b[0m         PerformanceWarning,\n\u001b[0;32m    134\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    135\u001b[0m     )\n\u001b[1;32m--> 137\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_selectors\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\reshape.py:189\u001b[0m, in \u001b[0;36m_Unstacker._make_selectors\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    186\u001b[0m mask\u001b[38;5;241m.\u001b[39mput(selector, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex):\n\u001b[1;32m--> 189\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndex contains duplicate entries, cannot reshape\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroup_index \u001b[38;5;241m=\u001b[39m comp_index\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask \u001b[38;5;241m=\u001b[39m mask\n",
      "\u001b[1;31mValueError\u001b[0m: Index contains duplicate entries, cannot reshape"
     ]
    }
   ],
   "source": [
    "# Define the directories\n",
    "current_directory = os.getcwd()\n",
    "sets_and_tags_directory = os.path.join(current_directory, '00_Sets&Tags')\n",
    "\n",
    "# Get a list of subdirectories in the current directory\n",
    "subdirectories_current = [d for d in os.listdir(current_directory) if os.path.isdir(os.path.join(current_directory, d)) and d.startswith(\"Par_\")]\n",
    "\n",
    "# Get list of csv files starting with Par_ from 00_Sets&Tags\n",
    "par_csv_files_sets_and_tags = [f for f in os.listdir(sets_and_tags_directory) if f.startswith('Par_') and f.endswith('.csv')]\n",
    "\n",
    "# For those CSV files, we'll treat their path as a \"subdirectory\" (even though they aren't directories)\n",
    "par_csv_filepaths_sets_and_tags = [os.path.join(sets_and_tags_directory, f) for f in par_csv_files_sets_and_tags]\n",
    "\n",
    "# Combine subdirectories from the current directory with filepaths from 00_Sets&Tags\n",
    "all_paths = subdirectories_current + par_csv_filepaths_sets_and_tags\n",
    "\n",
    "# Initialize the Excel writer\n",
    "output_excel_file_path = os.path.join(output_excel_directory, 'output.xlsx')\n",
    "with pd.ExcelWriter(output_excel_file_path, engine='openpyxl') as writer:\n",
    "    unique_values_concatenated.to_excel(writer, sheet_name='Sets', index=False, header=True)\n",
    "    \n",
    "    # Process CSV files in each path\n",
    "    for path in all_paths:\n",
    "        if os.path.isdir(path):\n",
    "            # If it's a directory, list all CSV files within\n",
    "            csv_files = [f for f in os.listdir(path) if f.endswith('.csv')]\n",
    "            csv_filepaths = [os.path.join(path, f) for f in csv_files]\n",
    "        else:\n",
    "            # If it's a CSV file, use it directly\n",
    "            csv_filepaths = [path]\n",
    "            \n",
    "        for csv_file_path in csv_filepaths:\n",
    "            # Compute and truncate worksheet_name to ensure it doesn't exceed 31 characters\n",
    "            worksheet_name = os.path.splitext(os.path.basename(csv_file_path))[0]\n",
    "            if len(worksheet_name) > 31:\n",
    "                worksheet_name = worksheet_name[:31]\n",
    "\n",
    "            # Read the CSV file into a Pandas DataFrame\n",
    "            df = pd.read_csv(csv_file_path, delimiter=',')\n",
    "            \n",
    "            # Create a list of columns to keep\n",
    "            columns_to_keep = [col for col in df.columns if col in unique_values_concatenated.columns or col == 'Value']\n",
    "            \n",
    "            # Filter the DataFrame to keep only the selected columns\n",
    "            df = df[columns_to_keep]\n",
    "            \n",
    "            pivot_indices = [col for col in unique_values_concatenated.columns if col in df.columns]\n",
    "            \n",
    "            \n",
    "        \n",
    "            # Iterate over unique_values_concatenated DataFrame columns\n",
    "            for header in unique_values_concatenated.columns:\n",
    "                if header in df.columns:\n",
    "                    # Filter the DataFrame based on whether the values in the header column are present in unique_values_concatenated\n",
    "                    df = df[df[header].isin(unique_values_concatenated[header])]\n",
    "\n",
    "                    \n",
    "            \n",
    "            # Store original dataframe for CSV output\n",
    "            df_original = df\n",
    "            \n",
    "            if 'Year' in df.columns:\n",
    "                print(csv_file_path)\n",
    "                year_column_position = df.columns.get_loc('Year')\n",
    "\n",
    "                # Select all columns before the 'Year' column as index columns\n",
    "                index_columns = df.columns[:year_column_position]\n",
    "                \n",
    "                full_duplicates = df[df.duplicated(keep=False)]\n",
    "                print(full_duplicates)\n",
    "\n",
    "                \n",
    "                # Pivot the DataFrame while setting the specified columns as the index\n",
    "                pivot_df = df.pivot(index=index_columns, columns='Year', values='Value').reset_index()\n",
    "                \n",
    "                print(pivot_df.head())\n",
    "\n",
    "   \n",
    "            # Formatting steps for both dataframes\n",
    "            df = pd.concat([df.columns.to_frame().T, df], ignore_index=True)\n",
    "            df.columns = range(len(df.columns))\n",
    "            df.replace('nan', '', inplace=True)\n",
    "            df.apply(lambda x: x.apply(lambda y: str(y).replace('.', ',')))\n",
    "            \n",
    "            df_original = pd.concat([df_original.columns.to_frame().T, df_original], ignore_index=True)\n",
    "            df_original.columns = range(len(df_original.columns))\n",
    "            df_original.replace('nan', '', inplace=True)\n",
    "            df_original.apply(lambda x: x.apply(lambda y: str(y).replace('.', ','))) \n",
    "            data_frames[worksheet_name] = df\n",
    "            \n",
    "            # Write original dataframe to CSV output\n",
    "            output_csv_file_path = os.path.join(output_csv_directory, os.path.basename(csv_file_path))\n",
    "            df_original.to_csv(output_csv_file_path, index=False, header=False, decimal='.')\n",
    "            \n",
    "            # Write pivoted dataframe to Excel output\n",
    "            df.to_excel(writer, sheet_name=worksheet_name, index=False, header=False)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c7d248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of subdirectories in the current directory\n",
    "subdirectories_TS = [d for d in os.listdir() if os.path.isdir(d) and d.startswith(\"TS_\")]\n",
    "# Initialize the Excel writer for hourly (TS_) files     \n",
    "output_excel_file_path_TS = os.path.join(output_excel_directory, 'output_TS.xlsx')\n",
    "with pd.ExcelWriter(output_excel_file_path_TS, engine='openpyxl') as writer:\n",
    "    # Process CSV files in each subdirectory\n",
    "    for subdirectory in subdirectories_TS:\n",
    "        # Logic to read in the hourly files and exporting the data for GENeSYS-MOD as .csv or excel file\n",
    "        csv_files = [f for f in os.listdir(subdirectory) if f.endswith('.csv')]\n",
    "         \n",
    "        for csv_file in csv_files:# Construct the full path to the CSV file\n",
    "            csv_file_path = os.path.join(subdirectory, csv_file)\n",
    "            \n",
    "            df_TS = pd.read_csv(csv_file_path, delimiter=',', skiprows=[0])\n",
    "            \n",
    "            # Create a DataFrame containing only the \"hour\" column\n",
    "            hour_column = df_TS[\"HOUR\"].to_frame()\n",
    "\n",
    "            #Create a list called \"selected_regions\" containing the unique values from the \"Region\" column\n",
    "            selected_regions_TS = unique_values_concatenated[\"Region\"].unique().tolist()\n",
    "\n",
    "            selected_regions_TS = [value for value in selected_regions_TS if not isinstance(value, float) or not math.isnan(value)]\n",
    "\n",
    "            #Filter the columns in the \"TS_WIND_ONSHORE_INF.csv\" DataFrame\n",
    "            filtered_df_TS = df_TS[selected_regions_TS]\n",
    "\n",
    "            # Concatenate the \"hour\" DataFrame as the first row of the filtered DataFrame\n",
    "            filtered_df_TS = pd.concat([hour_column, filtered_df_TS], axis=1, ignore_index=False)\n",
    "            \n",
    "            df_TS_final = filtered_df_TS\n",
    "            \n",
    "            # Get the worksheet name without the .csv extension\n",
    "            worksheet_name = os.path.splitext(csv_file)[0]\n",
    "            \n",
    "            # Store the DataFrame in the dictionary with the filename (without extension) as the key\n",
    "            data_frames[worksheet_name] = df_TS_final\n",
    "            \n",
    "            \n",
    "            # Write the DataFrame to an Excel worksheet with the same filename (without extension)\n",
    "            df_TS_final.to_excel(writer, sheet_name=worksheet_name, index=False, header=True)\n",
    "            \n",
    "            \n",
    "            # Specify the path where you want to save the CSV file\n",
    "            output_csv_file_path = os.path.join(output_csv_directory, csv_file)\n",
    "\n",
    "            # Use the to_csv method to save the DataFrame to a CSV file\n",
    "            df_TS_final.to_csv(output_csv_file_path, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7da714",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
