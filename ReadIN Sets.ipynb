{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43d3defb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "import math\n",
    "\n",
    "# Define the file to exclude\n",
    "excluded_file = 'Sets.csv'\n",
    "\n",
    "# Define the directory where you want to save the filtered CSV files\n",
    "output_csv_directory = 'output_csv'  # Change this to the directory where you want to save the filtered CSV files\n",
    "\n",
    "# Create the output directory for CSV files if it doesn't exist\n",
    "os.makedirs(output_csv_directory, exist_ok=True)\n",
    "\n",
    "# Define the directory where you want to save the Excel file\n",
    "output_excel_directory = 'output_excel'  # Change this to the directory where you want to save the Excel file\n",
    "\n",
    "# Create the output directory for the Excel file if it doesn't exist\n",
    "os.makedirs(output_excel_directory, exist_ok=True)\n",
    "\n",
    "# Specify the Excel file path\n",
    "excel_file_path = 'GENeSYS-MOD_User_Input_Settings_v06_kh_05-10-2023.xlsx'  # Replace with the path to your Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d170de21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the Excel file\n",
    "xls = pd.ExcelFile(excel_file_path, engine='openpyxl')\n",
    "\n",
    "# Get the list of sheet names in the Excel file\n",
    "sheets_to_read = xls.sheet_names\n",
    "\n",
    "# Initialize an empty dictionary to store DataFrames\n",
    "data_frames = {}\n",
    "filtered_df = {}\n",
    "unique_values = {}\n",
    "\n",
    "unique_values_concatenated = pd.DataFrame()\n",
    "column_list = []\n",
    "\n",
    "# Read sheets and store them in the dictionary\n",
    "for sheet_name in sheets_to_read:\n",
    "    data_frames = xls.parse(sheet_name)\n",
    "\n",
    "    filtered_df= data_frames[data_frames.iloc[:, 1] == 1] # Assuming the second column is indexed at 1 (0-based index)\n",
    "\n",
    "    column_list.append(filtered_df.columns[0]) # collect column header for each set sheet\n",
    "   \n",
    "    unique_values= pd.DataFrame(filtered_df.iloc[:, 0].unique())  # Assuming the first column is indexed at 0 (0-based index)\n",
    "    unique_values_parameter = pd.DataFrame(unique_values)\n",
    "    \n",
    "    unique_values_concatenated = pd.concat([unique_values_concatenated, unique_values], axis=1)\n",
    "\n",
    "# Close the Excel file\n",
    "xls.close()    \n",
    "    \n",
    "# Need to put header to the dataframe\n",
    "unique_values_concatenated.columns = column_list\n",
    "\n",
    "# Create a CSV file containing unique values\n",
    "unique_values_csv_file_path = os.path.join(output_csv_directory, 'Sets.csv')\n",
    "unique_values_concatenated.to_csv(unique_values_csv_file_path, index=False, decimal='.') \n",
    "\n",
    "if \"Region\" in unique_values_concatenated.columns:\n",
    "    unique_values_concatenated[\"Region2\"] = unique_values_concatenated[\"Region\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff37fca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty dictionary to store DataFrames\n",
    "data_frames = {}\n",
    "data_frames_to_write = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a45607b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everything worked! You are a hero!\n"
     ]
    }
   ],
   "source": [
    "# Define the directories\n",
    "current_directory = os.getcwd()\n",
    "sets_and_tags_directory = os.path.join(current_directory, '00_Sets&Tags')\n",
    "\n",
    "# Get a list of subdirectories in the current directory\n",
    "subdirectories_current = [d for d in os.listdir(current_directory) if os.path.isdir(os.path.join(current_directory, d)) and d.startswith(\"Par_\")]\n",
    "\n",
    "# Get list of csv files starting with Par_ from 00_Sets&Tags\n",
    "par_csv_files_sets_and_tags = [f for f in os.listdir(sets_and_tags_directory) if f.startswith('Par_') and f.endswith('.csv')]\n",
    "\n",
    "# For those CSV files, we'll treat their path as a \"subdirectory\" (even though they aren't directories)\n",
    "par_csv_filepaths_sets_and_tags = [os.path.join(sets_and_tags_directory, f) for f in par_csv_files_sets_and_tags]\n",
    "\n",
    "# Combine subdirectories from the current directory with filepaths from 00_Sets&Tags\n",
    "all_paths = subdirectories_current + par_csv_filepaths_sets_and_tags\n",
    "\n",
    "# Initialize the Excel writer\n",
    "output_excel_file_path = os.path.join(output_excel_directory, 'output.xlsx')\n",
    "with pd.ExcelWriter(output_excel_file_path, engine='openpyxl') as writer:\n",
    "    unique_values_concatenated.to_excel(writer, sheet_name='Sets', index=False, header=True)\n",
    "    \n",
    "\n",
    "    \n",
    "    # Process CSV files in each path\n",
    "    for path in all_paths:\n",
    "        if os.path.isdir(path):\n",
    "            # If it's a directory, list all CSV files within\n",
    "            csv_files = [f for f in os.listdir(path) if f.endswith('.csv')]\n",
    "            csv_filepaths = [os.path.join(path, f) for f in csv_files]\n",
    "        else:\n",
    "            # If it's a CSV file, use it directly\n",
    "            csv_filepaths = [path]\n",
    "            \n",
    "        for csv_file_path in csv_filepaths:\n",
    "            # Compute and truncate worksheet_name to ensure it doesn't exceed 31 characters\n",
    "            worksheet_name = os.path.splitext(os.path.basename(csv_file_path))[0]\n",
    "            if len(worksheet_name) > 31:\n",
    "                worksheet_name = worksheet_name[:31]\n",
    "\n",
    "            # Read the CSV file into a Pandas DataFrame\n",
    "            df = pd.read_csv(csv_file_path, delimiter=',')\n",
    "            \n",
    "            # Rename columns with .1, .2, etc. naming convention\n",
    "            for col in df.columns:\n",
    "                if '.' in col:\n",
    "                    base_name = col.split('.')[0]\n",
    "                    counter = int(col.split('.')[1]) + 1  # Add 1 because we start from the first duplicate\n",
    "                    new_col_name = f\"{base_name}{counter}\"\n",
    "                    df.rename(columns={col: new_col_name}, inplace=True)\n",
    "        \n",
    "            # Create a list of columns to keep\n",
    "            columns_to_keep = [col for col in df.columns if col in unique_values_concatenated.columns or col == 'Value']\n",
    "            \n",
    "            \n",
    "            # Filter the DataFrame to keep only the selected columns\n",
    "            df = df[columns_to_keep]       \n",
    "        \n",
    "            # Iterate over unique_values_concatenated DataFrame columns\n",
    "            for header in unique_values_concatenated.columns:\n",
    "                if header in df.columns:\n",
    "                    # Filter the DataFrame based on whether the values in the header column are present in unique_values_concatenated\n",
    "                    df = df[df[header].isin(unique_values_concatenated[header])]\n",
    "\n",
    "                    \n",
    "            \n",
    "            # Store original dataframe for CSV output\n",
    "            df_original = df\n",
    "            df_pivot = df\n",
    "            \n",
    "            # Assuming the 'Year' column exists in the df DataFrame or there's a \"Region2\" column\n",
    "            pivot_column = 'Year'\n",
    "            if 'Region2' in df.columns:\n",
    "                pivot_column = 'Region2'\n",
    "            \n",
    "            if pivot_column in df.columns:\n",
    "                # Check and print duplicates based on all columns\n",
    "                duplicates = df[df.duplicated(keep=False)]\n",
    "                if not duplicates.empty:\n",
    "                    print(worksheet_name)\n",
    "                    print(df)\n",
    "                    print(\"Duplicate entries:\")\n",
    "                    print(duplicates)\n",
    "                \n",
    "                # Identify other columns (excluding pivot_column and 'Value') to use as multi-index for pivoting\n",
    "                index_columns = [col for col in df.columns if col not in [pivot_column, 'Value']]\n",
    "                \n",
    "                # Pivot the DataFrame\n",
    "                df_pivot = df.pivot(index=index_columns, columns=pivot_column, values='Value').reset_index()\n",
    "                \n",
    "                # Convert MultiIndex columns (if any) back to single columns\n",
    "                df_pivot.columns = ['_'.join(map(str, col)).strip() if isinstance(col, tuple) else str(col) for col in df_pivot.columns.values]\n",
    "            \n",
    "            # Formatting steps for both dataframes\n",
    "            df_pivot = pd.concat([df_pivot.columns.to_frame().T, df_pivot], ignore_index=True)\n",
    "            #df_pivot.columns = range(len(df_pivot.columns))\n",
    "            df_pivot.replace('nan', '', inplace=True)\n",
    "            #df_pivot.apply(lambda x: x.apply(lambda y: str(y).replace('.', ',')))\n",
    "            \n",
    "            df_original = pd.concat([df_original.columns.to_frame().T, df_original], ignore_index=True)\n",
    "            df_original.columns = range(len(df_original.columns))\n",
    "            df_original.replace('nan', '', inplace=True)\n",
    "            df_original.apply(lambda x: x.apply(lambda y: str(y).replace('.', ','))) \n",
    "            data_frames[worksheet_name] = df\n",
    "            \n",
    "            # Write original dataframe to CSV output\n",
    "            output_csv_file_path = os.path.join(output_csv_directory, os.path.basename(csv_file_path))\n",
    "            df_original.to_csv(output_csv_file_path, index=False, header=False, decimal='.')\n",
    "            \n",
    "            ## Write pivoted dataframe to Excel output\n",
    "            #data_frames_to_write[worksheet_name] = df_original\n",
    "            data_frames_to_write[worksheet_name] = df_pivot\n",
    "            \n",
    "print(\"Everything worked! You are a hero!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7007f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After your for loop for all_paths ends:\n",
    "\n",
    "sorted_worksheet_names = sorted(data_frames_to_write.keys())\n",
    "with pd.ExcelWriter(output_excel_file_path, engine='openpyxl') as writer:\n",
    "    unique_values_concatenated.to_excel(writer, sheet_name='Sets', index=False, header=True)\n",
    "    \n",
    "    for worksheet_name in sorted_worksheet_names:\n",
    "        data_frames_to_write[worksheet_name].to_excel(writer, sheet_name=worksheet_name, index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c7d248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of subdirectories in the current directory\n",
    "subdirectories_TS = [d for d in os.listdir() if os.path.isdir(d) and d.startswith(\"TS_\")]\n",
    "# Initialize the Excel writer for hourly (TS_) files     \n",
    "output_excel_file_path_TS = os.path.join(output_excel_directory, 'output_TS.xlsx')\n",
    "with pd.ExcelWriter(output_excel_file_path_TS, engine='openpyxl') as writer:\n",
    "    # Process CSV files in each subdirectory\n",
    "    for subdirectory in subdirectories_TS:\n",
    "        # Logic to read in the hourly files and exporting the data for GENeSYS-MOD as .csv or excel file\n",
    "        csv_files = [f for f in os.listdir(subdirectory) if f.endswith('.csv')]\n",
    "         \n",
    "        for csv_file in csv_files:# Construct the full path to the CSV file\n",
    "            csv_file_path = os.path.join(subdirectory, csv_file)\n",
    "            \n",
    "            df_TS = pd.read_csv(csv_file_path, delimiter=',', skiprows=[0])\n",
    "            \n",
    "            # Create a DataFrame containing only the \"hour\" column\n",
    "            hour_column = df_TS[\"HOUR\"].to_frame()\n",
    "\n",
    "            # Create a list called \"selected_regions\" containing the unique values from the \"Region\" column\n",
    "            selected_regions_TS = unique_values_concatenated[\"Region\"].unique().tolist()\n",
    "            \n",
    "            selected_regions_TS = [value for value in selected_regions_TS if not isinstance(value, float) or not math.isnan(value)]\n",
    "            \n",
    "            # Cross-check if columns in selected_regions_TS are present in df_TS\n",
    "            selected_regions_TS = [region for region in selected_regions_TS if region in df_TS.columns]\n",
    "            \n",
    "            #Filter the columns in the \"TS_WIND_ONSHORE_INF.csv\" DataFrame\n",
    "            filtered_df_TS = df_TS[selected_regions_TS]\n",
    "            \n",
    "\n",
    "            # Concatenate the \"hour\" DataFrame as the first row of the filtered DataFrame\n",
    "            filtered_df_TS = pd.concat([hour_column, filtered_df_TS], axis=1, ignore_index=False)\n",
    "            \n",
    "            df_TS_final = filtered_df_TS\n",
    "            \n",
    "            # Get the worksheet name without the .csv extension\n",
    "            worksheet_name = os.path.splitext(csv_file)[0]\n",
    "            \n",
    "            # Store the DataFrame in the dictionary with the filename (without extension) as the key\n",
    "            data_frames[worksheet_name] = df_TS_final\n",
    "            \n",
    "            \n",
    "            # Write the DataFrame to an Excel worksheet with the same filename (without extension)\n",
    "            df_TS_final.to_excel(writer, sheet_name=worksheet_name, index=False, header=True)\n",
    "            \n",
    "            \n",
    "            # Specify the path where you want to save the CSV file\n",
    "            output_csv_file_path = os.path.join(output_csv_directory, csv_file)\n",
    "\n",
    "            # Use the to_csv method to save the DataFrame to a CSV file\n",
    "            df_TS_final.to_csv(output_csv_file_path, index=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7da714",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
